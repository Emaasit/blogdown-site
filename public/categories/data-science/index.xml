<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science on Daniel Emaasit</title>
    <link>/categories/data-science/</link>
    <description>Recent content in Data Science on Daniel Emaasit</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Daniel Emaasit</copyright>
    <lastBuildDate>Mon, 19 Mar 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/data-science/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Gaussian Processes with Spectral Mixture Kernels to Implicitly Capture Hidden Structure from Data</title>
      <link>/post/2018/03/19/gaussian-processes-with-spectral-mixture-kernels-to-implicitly-capture-hidden-structure-from-data/</link>
      <pubDate>Mon, 19 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/03/19/gaussian-processes-with-spectral-mixture-kernels-to-implicitly-capture-hidden-structure-from-data/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;(Note: Cross-posted with the &lt;a href=&#34;https://wp.me/p7rVtH-1Fv&#34; target=&#34;_blank&#34;&gt;Haystax Technology Blog&lt;/a&gt;.)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Several scientific fields such as insider-threat detection, highway-safety planning, often lack sufficient amounts of time-series training data for the purpose of scientific discovery. Moreover, the available limited data are quite noisy. For instance Greitzer and Ferryman (2013) state that ”ground truth” data on actual insider behavior is typically either not available or is limited. In some cases, one might acquire real data, but for privacy reasons, there is no attribution of any individuals relating to abuses or offenses i.e., there is no ground truth. The data may contain insider threats, but these are not identified or knowable to the researcher (Greitzer and Ferryman, 2013; Gheyas and Abdallah, 2016).In highway-safety
planning, Veeramisti (2016) mentions that Departments of Transportation (DOTs) only recently started collecting monthly highway-crash data because of the high cost and extensive process of collecting the required data.&lt;/p&gt;

&lt;h1 id=&#34;the-problem&#34;&gt;The Problem&lt;/h1&gt;

&lt;p&gt;Having limited and quite noisy data for insider-threat detection and highway-safety planning presents a major challenge when estimating time-series models that are robust to overfitting and have well-calibrated uncertainty estimates. Most of the current literature in time-series modeling in these scientific fields is associated with two major limitations.&lt;/p&gt;

&lt;p&gt;First, the methods involve visualizing the time series for noticeable structure and patterns such as periodicity, smoothness, growing/decreasing trends and then hard-coding these patterns into the statistical models during formulation. This approach is suitable for large datasets where more data typically provides more information to learn expressive structure. Given limited amounts of data, such expressive structure may not be easily noticeable. For instance, the figure below shows monthly attachment size in emails (in Gigabytes) sent by an insider from their employee account to their home account. Trends such as periodicity, smoothness, growing/decreasing trends are not easily noticeable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/data-emails.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Second, most of the current literature focuses on parametric models that impose strong restrictive assumptions by pre-specifying the functional form and number of parameters. Pre-specifying a functional form for a time-series model could lead to either overly complex model specifications or simplistic models. It is difficult to know &lt;em&gt;a priori&lt;/em&gt; the most appropriate function to use for modeling sophisticated insider-threat behavior or highway-crash scenarios that involve complex hidden patterns and many other influencing factors.&lt;/p&gt;

&lt;h2 id=&#34;source-code&#34;&gt;Source code&lt;/h2&gt;

&lt;p&gt;For the impatient reader, two options are provided below to access the source code used for empirical analyses:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The entire project (code, notebooks, data, and results) can be found &lt;a href=&#34;https://github.com/Emaasit/long-range-extrapolation&#34; target=&#34;_blank&#34;&gt;here on GitHub&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click the binder icon below to open the notebooks in a web browser and explore the entire project without downloading and installing any software.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/Emaasit/long-range-extrapolation/master?urlpath=lab&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg&#34; alt=&#34;Binder&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;data-science-questions&#34;&gt;Data Science Questions&lt;/h1&gt;

&lt;p&gt;Given the above limitations in the current state-of-art, this study formulated the following three Data Science questions. Given limited and quite noisy time-series data for insider-threat detection and highway-safety planning, is it possible to perform:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;pattern discovery without hard-coding trends into statistical models during formulation?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;model estimation that precludes pre-specifying a functional form?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;model estimation that is robust to overfitting and has well-calibrated uncertainty estimates?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;hypothesis&#34;&gt;Hypothesis&lt;/h1&gt;

&lt;p&gt;To answer these three Data Science questions and address the above-described limitations, this study formulated the following hypothesis:
&lt;blockquote&gt;This study hypothesizes that by leveraging current state-of-the-art innovations in nonparametric Bayesian methods, such as Gaussian processes with spectral mixture kernels, it is possible to perform pattern discovery without prespecifying functional forms and hard-coding trends into statistical models.&lt;/blockquote&gt;&lt;/p&gt;

&lt;h1 id=&#34;methodology&#34;&gt;Methodology&lt;/h1&gt;

&lt;p&gt;To test the above hypothesis, a nonparametric Bayesian approach was proposed to implicitly capture hidden structure from time series having limited data. The proposed model, a Gaussian process with a spectral mixture kernel, precludes the need to pre-specify a functional form and hard code trends, is robust to overfitting and has well-calibrated uncertainty estimates.&lt;/p&gt;

&lt;p&gt;Mathematical details of the proposed model formulation are described in a corresponding paper that can be found on arXiv through the link below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Emaasit, D. and Johnson, M. (2018). &lt;a href=&#34;https://arxiv.org/abs/1803.05867&#34; target=&#34;_blank&#34;&gt;Capturing Structure Implicitly from Noisy Time-Series having Limited Data&lt;/a&gt;. arXiv preprint arXiv:1803.05867.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A Brief description of the fundamental concepts of the proposed methodology is as follows. Consider for each data point, $i$, that $y_i$ represents the attachment size in emails sent by an insider to their home account and $x_i$ is a temporal covariate such as month. The task is to estimate a latent function $f$, which maps input data, $x_i$, to output data $y_i$ for $i$ = 1, 2, $\ldots{}$, $N$, where $N$ is the total number of data points. Each of the input data $x_i$ is of a single dimension $D = 1$, and $\textbf{X}$ is a $N$ x $D$ matrix with rows $x_i$.&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;size-medium wp-image-6429 aligncenter&#34; src=&#34;http://haystax.com/wp-content/uploads/2018/03/gp-pgm-352x300.png&#34; alt=&#34;&#34; width=&#34;352&#34; height=&#34;200&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The observations are assumed to satisfy:&lt;/p&gt;

&lt;p&gt;\begin{equation}
y_i = f(x&lt;em&gt;i) + \varepsilon, \quad where \, \, \varepsilon \sim \mathcal{N}(0, \sigma&lt;/em&gt;{\varepsilon}^2)
\end{equation}&lt;/p&gt;

&lt;p&gt;The noise term, $\varepsilon$, is assumed to be normally distributed with a zero mean and variance, $\sigma_{\varepsilon}^2$. Latent function $f$ represents hidden underlying trends that produced the observed time-series data.&lt;/p&gt;

&lt;p&gt;Given that it is difficult to know $\textit{a priori}$ the most appropriate functional form to use for $f$, a prior distribution, $p(\textbf{f})$, over an infinite number of possible functions of interest is formulated. A natural prior over an infinite space of functions is a Gaussian process prior (Williams and Rasmussen, 2006). A GP is fully parameterized by a mean function, $\textbf{m}$, and covariance function, $\textbf{K}_{N,N}$, denoted as:&lt;/p&gt;

&lt;p&gt;\begin{equation}\label{eqn:gpsim}
\textbf{f} \sim \mathcal{GP}(\textbf{m}, \textbf{K}_{N,N}),
\end{equation}&lt;/p&gt;

&lt;p&gt;The posterior distribution over the unknown function evaluations, $\textbf{f}$, at all data points, $x_i$, was estimated using Bayes theorem as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}\label{eqn:bayesinfty}
\begin{aligned}
p(\textbf{f} \mid \textbf{y},\textbf{X}) = \frac{p(\textbf{y} \mid \textbf{f}, \textbf{X}) \, p(\textbf{f})}{p(\textbf{y} \mid \textbf{X})} &lt;br /&gt;
= \frac{p(\textbf{y} \mid \textbf{f}, \textbf{X}) \, \mathcal{N}(\textbf{f} \mid \textbf{m}, \textbf{K}_{N,N})}{p(\textbf{y} \mid \textbf{X})},
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;p&gt;\begin{aligned}
p(\textbf{f}\mid \textbf{y},\textbf{X}) = \text{the posterior distribution of functions that best explain the response variable, given the covariates}
\end{aligned}&lt;/p&gt;

&lt;p&gt;\begin{aligned}
p(\textbf{y} \mid \textbf{f}, \textbf{X}) = \text{the likelihood of response variable, given the functions and covariates}&lt;br /&gt;
\end{aligned}&lt;/p&gt;

&lt;p&gt;\begin{aligned}
p(\textbf{f}) = \text{the prior over all possible functions of the response variable}
\end{aligned}&lt;/p&gt;

&lt;p&gt;\begin{aligned}
p(\textbf{y} \mid \textbf{X}) = \text{the data (constant)}&lt;br /&gt;
\end{aligned}&lt;/p&gt;

&lt;p&gt;A spectral mixture kernel was proposed for the covariance function, $\textbf{K}_{N,N}$. The resulting posterior, $p(\textbf{f}\mid \textbf{y},\textbf{X})$ is a Gaussian process composed of a distribution of possible functions that best explain the time-series pattern.&lt;/p&gt;

&lt;h1 id=&#34;experiments&#34;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&#34;the-setup&#34;&gt;The setup&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s first install some python packages that we shall use for our analysis. Also we shall set up our plotting requirements.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_context(&#39;notebook&#39;, font_scale = 1.1)
np.random.seed(12345)
rc = {&#39;xtick.labelsize&#39;: 40, &#39;ytick.labelsize&#39;: 40, &#39;axes.labelsize&#39;: 40, &#39;font.size&#39;: 40, &#39;lines.linewidth&#39;: 4.0, 
      &#39;lines.markersize&#39;: 40, &#39;font.family&#39;: &amp;quot;serif&amp;quot;, &#39;font.serif&#39;: &amp;quot;cm&amp;quot;, &#39;savefig.dpi&#39;: 200,
      &#39;text.usetex&#39;: False, &#39;legend.fontsize&#39;: 40.0, &#39;axes.titlesize&#39;: 40, &amp;quot;figure.figsize&amp;quot;: [24, 16]}
sns.set(rc = rc)
sns.set_style(&amp;quot;darkgrid&amp;quot;)
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = &amp;quot;all&amp;quot;
import gpflow
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;raw-data-and-sample-formation&#34;&gt;Raw data and sample formation&lt;/h2&gt;

&lt;p&gt;The insider-threat data used for empirical analysis in this study was provided by the computer emergency response team (CERT) division of the software engineering institute (SEI) at Carnegie Mellon University. The particular insider threat focused on is the case where a known insider sent information as email attachments from their work email to their home email.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s read in the data using &lt;code&gt;pandas&lt;/code&gt;, view the first three records and the structure of the resulting &lt;code&gt;pandas&lt;/code&gt; dataframe.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;email_filtered = pd.read_csv(&amp;quot;../data/emails/email_filtered.csv&amp;quot;, parse_dates=[&amp;quot;date&amp;quot;])
email_filtered.head(n = 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;user&lt;/th&gt;
      &lt;th&gt;pc&lt;/th&gt;
      &lt;th&gt;to&lt;/th&gt;
      &lt;th&gt;cc&lt;/th&gt;
      &lt;th&gt;bcc&lt;/th&gt;
      &lt;th&gt;from&lt;/th&gt;
      &lt;th&gt;activity&lt;/th&gt;
      &lt;th&gt;size&lt;/th&gt;
      &lt;th&gt;attachments&lt;/th&gt;
      &lt;th&gt;content&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;{D0V4-N9KM15BF-0512LLVP}&lt;/td&gt;
      &lt;td&gt;2010-01-04 07:36:48&lt;/td&gt;
      &lt;td&gt;BTR2026&lt;/td&gt;
      &lt;td&gt;PC-9562&lt;/td&gt;
      &lt;td&gt;Thaddeus.Brett.Daniel@dtaa.com&lt;/td&gt;
      &lt;td&gt;Zorita.Angela.Wilson@dtaa.com&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Beau.Todd.Romero@dtaa.com&lt;/td&gt;
      &lt;td&gt;Send&lt;/td&gt;
      &lt;td&gt;23179&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;On November 25, general Savary was sent to the...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;{L5E5-J1HB80OY-9539AOEC}&lt;/td&gt;
      &lt;td&gt;2010-01-04 07:38:18&lt;/td&gt;
      &lt;td&gt;BTR2026&lt;/td&gt;
      &lt;td&gt;PC-9562&lt;/td&gt;
      &lt;td&gt;Beau.Todd.Romero@dtaa.com&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Marsh_Travis@raytheon.com&lt;/td&gt;
      &lt;td&gt;View&lt;/td&gt;
      &lt;td&gt;17047&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Early in the morning of May 27, a boat crossed...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;{Q4V7-V6BR00TZ-5209UVDX}&lt;/td&gt;
      &lt;td&gt;2010-01-04 07:53:35&lt;/td&gt;
      &lt;td&gt;BTR2026&lt;/td&gt;
      &lt;td&gt;PC-9562&lt;/td&gt;
      &lt;td&gt;Bianca-Clark@optonline.net&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Beau_Romero@aol.com&lt;/td&gt;
      &lt;td&gt;Send&lt;/td&gt;
      &lt;td&gt;26507&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;The Americans never held up their side of the ...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Our &lt;code&gt;pandas&lt;/code&gt; dataframe comprises columns that we are interested in such as &amp;ldquo;user&amp;rdquo; (username), &amp;ldquo;date&amp;rdquo;, &amp;ldquo;to&amp;rdquo; (the recipient email address) and &amp;ldquo;size&amp;rdquo; (attachment size) of emails.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;email_filtered.info()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 11920 entries, 0 to 11919
Data columns (total 12 columns):
id             11920 non-null object
date           11920 non-null datetime64[ns]
user           11920 non-null object
pc             11920 non-null object
to             11920 non-null object
cc             6101 non-null object
bcc            593 non-null object
from           11920 non-null object
activity       11920 non-null object
size           11920 non-null int64
attachments    3809 non-null object
content        11920 non-null object
dtypes: datetime64[ns](1), int64(1), object(10)
memory usage: 1.1+ MB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s filter data for a particular known insider with user ID &amp;ldquo;CDE1846&amp;rdquo; and summarize the total attachment size of emails by month. This results into 17 data points ranging from January 2010 to May 2011.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_insider = email_filtered[email_filtered[&amp;quot;user&amp;quot;] == &amp;quot;CDE1846&amp;quot;]
emails_per_month = df_insider.resample(rule = &amp;quot;1M&amp;quot;, on = &amp;quot;date&amp;quot;).sum().reset_index()
emails_per_month[&amp;quot;date&amp;quot;] = pd.to_datetime(emails_per_month[&amp;quot;date&amp;quot;], format = &amp;quot;%Y-%m-%d&amp;quot;)
emails_per_month.columns = [&amp;quot;ds&amp;quot;, &amp;quot;y&amp;quot;]
emails_per_month.y = emails_per_month.y/1e6
emails_per_month[&amp;quot;ds&amp;quot;] = emails_per_month[&amp;quot;ds&amp;quot;].apply(lambda x: x.strftime(&#39;%Y-%m&#39;)).astype(str)
emails_per_month
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ds&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2010-01&lt;/td&gt;
      &lt;td&gt;117.809274&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2010-02&lt;/td&gt;
      &lt;td&gt;112.461320&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2010-03&lt;/td&gt;
      &lt;td&gt;134.592245&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2010-04&lt;/td&gt;
      &lt;td&gt;148.911866&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2010-05&lt;/td&gt;
      &lt;td&gt;80.689085&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2010-06&lt;/td&gt;
      &lt;td&gt;128.024029&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;2010-07&lt;/td&gt;
      &lt;td&gt;115.046041&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;2010-08&lt;/td&gt;
      &lt;td&gt;142.607937&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;2010-09&lt;/td&gt;
      &lt;td&gt;121.728119&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;2010-10&lt;/td&gt;
      &lt;td&gt;126.041467&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;2010-11&lt;/td&gt;
      &lt;td&gt;80.338345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;2010-12&lt;/td&gt;
      &lt;td&gt;113.142879&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;2011-01&lt;/td&gt;
      &lt;td&gt;95.485553&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;2011-02&lt;/td&gt;
      &lt;td&gt;88.279993&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;2011-03&lt;/td&gt;
      &lt;td&gt;148.193802&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;2011-04&lt;/td&gt;
      &lt;td&gt;221.337135&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;2011-05&lt;/td&gt;
      &lt;td&gt;146.220533&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Let&amp;rsquo;s visualize this data using &lt;code&gt;matplotlib&lt;/code&gt; and &lt;code&gt;seaborn&lt;/code&gt;. The resulting figure shows no interesting insights just yet.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.barplot(data = emails_per_month, x = &amp;quot;ds&amp;quot;, y = &amp;quot;y&amp;quot;, color = &amp;quot;blue&amp;quot;, saturation = .5)
ax.set_xticklabels(labels = emails_per_month.ds, rotation = 45)
ax.set_xlabel(&#39;Time&#39;)
ax.set_ylabel(&#39;Total size of emails in GB&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/output_12_0.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s look at the case where the insider sent email IP from their employee account to their home account. Visualizing this data shows some interesting trends towards the end of the analysis period. The attachment size increases drastically in March and April of 2011.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_insider_non_org = df_insider[~df_insider[&#39;to&#39;].str.contains(&#39;dtaa.com&#39;)]
df_insider_ewing = df_insider_non_org[df_insider_non_org[&#39;to&#39;] == &#39;Ewing_Carlos@comcast.net&#39;]
df = df_insider_ewing.resample(&#39;1M&#39;, on=&#39;date&#39;).sum().reset_index()
df.columns = [&amp;quot;ds&amp;quot;, &amp;quot;y&amp;quot;]
df.y = df.y/1e6
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;ds&amp;quot;] = df[&amp;quot;ds&amp;quot;].apply(lambda x: x.strftime(&#39;%Y-%m&#39;)).astype(str)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.barplot(data = df, x = &amp;quot;ds&amp;quot;, y = &amp;quot;y&amp;quot;, color = &amp;quot;blue&amp;quot;, saturation = .5)
ax.set_xticklabels(labels = df.ds, rotation = 45)
ax.set_xlabel(&#39;Time&#39;)
ax.set_ylabel(&#39;Total size of emails in GB&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/output_16_0.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;empirical-analysis&#34;&gt;Empirical analysis&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s drop the anormalous data points from our dataframe so that we can train a model for the normal behaviour and then create a training dataset of size = 11 and remaining data our for testing the estimated model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.drop([14, 15, 16])
test_size = 11
X_complete = np.array([df.index]).reshape((df.shape[0], 1)).astype(&#39;float64&#39;)
X_train = X_complete[0:test_size, ]
X_test = X_complete[test_size:df.shape[0], ]
Y_complete = np.array([df.y]).reshape((df.shape[0], 1)).astype(&#39;float64&#39;)
Y_train = Y_complete[0:test_size, ]
Y_test = Y_complete[test_size:df.shape[0], ]
D = Y_train.shape[1];
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
ax.plot(X_train.flatten(),Y_train.flatten(), c =&#39;k&#39;, marker = &amp;quot;o&amp;quot;, label = &amp;quot;Training data&amp;quot;)
ax.plot(X_test.flatten(),Y_test.flatten(), c=&#39;b&#39;, marker = &amp;quot;o&amp;quot;, label = &#39;Test data&#39;)
ax.set_xticks(ticks = df.index)
ax.set_xticklabels(labels = df.ds, rotation = 45)
ax.set_xlabel(&#39;Time&#39;)
ax.set_ylabel(&#39;Total size of emails in GB&#39;)
plt.legend(loc = &amp;quot;best&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/output_20_0.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s now develop a Gaussian Process model with a Spectral Mixture (SM) kernel proposed by Wilson (2014). This is because the SM kernel is capable of capturing hidden structure with data without hard cording features in a kernel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Trains a model with a spectral mixture kernel, given an ndarray of 
# 2Q frequencies and lengthscales

Q = 10 # nr of terms in the sum
max_iters = 1000

def create_model(hypers):
    f = np.clip(hypers[:Q], 0, 5)
    weights = np.ones(Q) / Q
    lengths = hypers[Q:]

    kterms = []
    for i in range(Q):
        rbf = gpflow.kernels.RBF(D, lengthscales=lengths[i], variance=1./Q)
        rbf.lengthscales.transform = gpflow.transforms.Exp()
        cos = gpflow.kernels.Cosine(D, lengthscales=f[i])
        kterms.append(rbf * cos)

    k = np.sum(kterms) + gpflow.kernels.Linear(D) + gpflow.kernels.Bias(D)
    m = gpflow.gpr.GPR(X_train, Y_train, kern=k)
    return m

m = create_model(np.ones((2*Q,)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s perfrom inference through optimization of the likelihood.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
m.optimize(maxiter = max_iters)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 7.74 s, sys: 289 ms, total: 8.03 s
Wall time: 7.61 s


      fun: 20.868585670810997
 hess_inv: &amp;lt;43x43 LbfgsInvHessProduct with dtype=float64&amp;gt;
      jac: array([  8.99958679e-06,   1.41339465e-05,   5.09060783e-05,
         6.72588106e-06,  -1.28315446e-08,   1.22652879e-05,
         5.09060783e-05,   6.72588106e-06,  -1.28315446e-08,
         1.22652879e-05,   5.09060783e-05,   6.72588106e-06,
        -1.28315446e-08,   1.22652879e-05,   5.09060783e-05,
         6.72588106e-06,  -1.28315446e-08,   1.22652879e-05,
         5.09060783e-05,   6.72588106e-06,  -1.28315446e-08,
         1.22652879e-05,   5.09060783e-05,   6.72588106e-06,
        -1.28315446e-08,   1.22652879e-05,   5.09060783e-05,
         6.72588106e-06,  -1.28315446e-08,   1.22652879e-05,
         5.09060783e-05,   6.72588106e-06,  -1.28315446e-08,
         1.22652879e-05,   5.09060783e-05,   6.72588106e-06,
        -1.28315446e-08,   1.22652879e-05,   5.09060783e-05,
         6.72588106e-06,  -1.28315446e-08,   1.22652879e-05,
         5.10663145e-06])
  message: b&#39;CONVERGENCE: REL_REDUCTION_OF_F_&amp;lt;=_FACTR*EPSMCH&#39;
     nfev: 50
      nit: 42
   status: 0
  success: True
        x: array([  2.1321322 ,  -1.88610378,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.26608891])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plotprediction(m):
    # Perform prediction
    mu, var = m.predict_f(X_complete)

    # Plot
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.set_xticks(ticks = df.index)
    ax.set_xticklabels(labels = df.ds, rotation = 45)
    ax.set_xlabel(&#39;Time&#39;)
    ax.set_ylabel(&#39;Total size of emails in GB&#39;);
    ax.plot(X_train.flatten(),Y_train.flatten(), c=&#39;k&#39;, marker = &amp;quot;o&amp;quot;, label = &#39;Training data&#39;)
    ax.plot(X_test.flatten(),Y_test.flatten(), c=&#39;b&#39;, marker = &amp;quot;o&amp;quot;, label = &#39;Test data&#39;)
    ax.plot(X_complete.flatten(), mu.flatten(), c=&#39;g&#39;, marker = &amp;quot;o&amp;quot;, label = &amp;quot;Predicted mean function&amp;quot;)
    lower = mu - 2*np.sqrt(var)
    upper = mu + 2*np.sqrt(var)
    ax.plot(X_complete, upper, &#39;g--&#39;, X_complete, lower, &#39;g--&#39;, lw=1.2)
    ax.fill_between(X_complete.flatten(), lower.flatten(), upper.flatten(),
                    color=&#39;g&#39;, alpha=.1, label = &amp;quot;95% Predicted credible interval&amp;quot;)
    plt.legend(loc = &amp;quot;best&amp;quot;)
    plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plotprediction(m);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/output_26_0.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Figure above shows that the Gaussian process model with a spectral mixture kernel is able to capture the structure both in regions of the training and testing data. The 95% predicted credible interval (CI) contains the &amp;ldquo;normal&amp;rdquo; size of email attachments for the duration of the measurements.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s calculate some performance measures such as the Root Mean Square Error (RMSE) and Mean Absolute Performance Error (MAPE).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## Calculate the RMSE and MAPE
def calculate_rmse(model, X_test, Y_test):
    mu, var = model.predict_y(X_test)
    rmse = np.sqrt(((mu - Y_test)**2).mean())
    return rmse

def calculate_mape(model, X_test, Y_test):
    mu, var = model.predict_y(X_test)
    mape = (np.absolute(((mu - Y_test)/Y_test)*100)).mean()
    return mape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;calculate_rmse(model=m, X_test = X_test, Y_test = Y_test)
calculate_mape(model=m, X_test = X_test, Y_test = Y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1.2515806168637664
27.94536660649003
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s estimate an ARIMA model was estimated using the &lt;code&gt;statsmodels&lt;/code&gt; package for comparison.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import itertools
import numpy.ma as ma
import warnings
from statsmodels.tsa.arima_model import ARIMA
from numpy.linalg import LinAlgError


def get_ARIMA_param_values(y):
    &amp;quot;&amp;quot;&amp;quot; Get best ARIMA values given data
    &amp;quot;&amp;quot;&amp;quot;
    warnings.filterwarnings(&#39;ignore&#39;)
    
    # Values to try
    p = [0, 1, 2, 3, 4, 5, 6]
    d = [0, 1, 2]
    q = [0, 1, 2, 3, 4, 5, 6]
    results = []

    for pi, di, qi in itertools.product(p, d, q):
        try:
            model = ARIMA(y, order=(pi, di, qi))
            model_fit = model.fit()
            aic = model_fit.aic
            if not np.isnan(aic):
                results.append(((pi,di,qi), aic, model_fit))
        except ValueError:
            pass
        except LinAlgError:
            pass
    warnings.filterwarnings(&#39;default&#39;)
    return sorted(results, key=lambda x: x[1])[0]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make prediction
steps = X_test.shape[0]
params, aic, model_fit = get_ARIMA_param_values(y = Y_train)
mu, stderr, conf_int = model_fit.forecast(steps = steps, alpha=0.05)
params, aic, mu, stderr, conf_int
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;((0, 1, 0),
 47.811434155792767,
 array([ 6.475238,  7.047388,  7.619538]),
 array([ 2.16329641,  3.05936312,  3.7469393 ]),
 array([[  2.23525495,  10.71522105],
        [  1.05114646,  13.04362954],
        [  0.27567193,  14.96340407]]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plotprediction_arima(m):
    mu, stderr, conf_int = m.forecast(steps=steps, alpha=0.05)

    # Plot
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.set_xticks(ticks = df.index)
    ax.set_xticklabels(labels = df.ds, rotation = 45)
    ax.set_xlabel(&amp;quot;Time&amp;quot;)
    ax.set_ylabel(&amp;quot;Total size of emails in GB&amp;quot;);
    ax.plot(X_train.flatten(), Y_train.flatten(), c=&#39;k&#39;, marker = &amp;quot;o&amp;quot;, label = &#39;Training data&#39;)
    ax.plot(X_test.flatten(), Y_test.flatten(), c=&#39;b&#39;, marker = &amp;quot;o&amp;quot;, label = &#39;Test data&#39;)
    ax.plot(X_test.flatten(), mu.flatten(), c=&#39;g&#39;, marker = &amp;quot;o&amp;quot;, label = &amp;quot;Predicted value&amp;quot;)
    lower = conf_int[:,0 ]
    upper = conf_int[:,1 ]
    ax.plot(X_test, upper, &#39;g--&#39;, X_test, lower, &#39;g--&#39;, lw=1.2)
    ax.fill_between(X_test.flatten(), lower.flatten(), upper.flatten(),
                    color=&#39;g&#39;, alpha=.1, label = &amp;quot;95% confidence interval&amp;quot;)
    plt.legend(loc = &amp;quot;best&amp;quot;)
    plt.tight_layout()

plotprediction_arima(m = model_fit);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/output_34_0.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Figure above shows that the ARIMA model is poor at capturing the structure within the region of testing data. This finding suggests that ARIMA models have poor performance for small data without noticeable structure. The 95% confidence interval for ARIMA is much wider than the GP model showing a high degree of uncertainty about the ARIMA predictions.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s calculate some performance measures such as the RMSE and MAPE for the ARIMA model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## Calculate the RMSE and MAPE
def calculate_rmse_arima(mu, Y_test):
    rmse = np.sqrt(((mu - Y_test)**2).mean())
    return rmse

def calculate_mape_arima(mu, Y_test):
    mape = (np.absolute(((mu - Y_test)/Y_test)*100)).mean()
    return mape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;calculate_rmse_arima(mu = mu, Y_test = Y_test)
calculate_mape_arima(mu = mu, Y_test = Y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;3.2019469508164868
93.44165723388285
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The ARIMA model has a poor predictive performance compared to the Gaussian process model with a spectral mixture kernel&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;This study proposed a Bayesian nonparametric framework to capture implicitly hidden structure in time-series having limited data. The proposed framework, a Gaussian process with a spectral mixture kernel, was applied to time-series process for insider-threat data. The proposed framework addresses two current challenges when analyzing quite noisy time-series having limited data whereby the time series are visualized for noticeable structure such as periodicity, growing or decreasing trends and hard coding them into pre-specified functional forms. Experiments demonstrated that results from this framework outperform traditional ARIMA when the time series does not have easily noticeable structure and is quite noisy. Future work will involve evaluating the proposed framework on other different types of insider-threat behavior.&lt;/p&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Emaasit, D. and Johnson, M. (2018). Capturing Structure Implicitly from Noisy Time-Series having Limited Data. arXiv preprint arXiv:1803.05867.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Williams, C. K. and Rasmussen, C. E. (2006). Gaussian processes for machine learning. the MIT Press, 2(3):4.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Knudde, N., van der Herten, J., Dhaene, T., &amp;amp; Couckuyt, I. (2017). GPflowOpt: A Bayesian Optimization Library using TensorFlow. arXiv preprint arXiv:1711.03845.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Wilson, A. G. (2014). Covariance kernels for fast automatic pattern discovery and extrapolation with Gaussian processes. University of Cambridge.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Greitzer, F. L. and Ferryman, T. A. (2013). Methods and metrics for evaluating analytic insider threat tools. In Security and Privacy Workshops (SPW), 2013 IEEE, pages 90–97. IEEE.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gheyas, I. A. and Abdallah, A. E. (2016). Detection and prediction of insider threats to cybersecurity: a systematic literature review and meta-analysis. Big Data Analytics, 1(1):6.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Veeramisti, N. K. (2016). A business intelligence framework for network-level traffic safety analyses. PhD thesis, University of Nevada, Las Vegas.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;computing-environment&#34;&gt;Computing Environment&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# print system information/setup
%reload_ext watermark
%watermark -v -m -p numpy,pandas,gpflowopt,gpflow,tensorflow,matplotlib,ipywidgets,seaborn -g
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPython 3.6.3
IPython 6.2.1

numpy 1.13.3
pandas 0.20.3
gpflowopt 0.1.0
gpflow 0.4.0
tensorflow 1.4.1
matplotlib 2.1.1
ipywidgets 7.1.1
seaborn 0.8.1

compiler   : GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)
system     : Darwin
release    : 17.3.0
machine    : x86_64
processor  : i386
CPU cores  : 8
interpreter: 64bit
Git hash   : df453b6da183c9f8fd941eaa1696e68f9731c771
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ICYMI: My talk on Introduction to Probabilistic Machine Learning</title>
      <link>/post/2017/08/14/introduction-to-probabilistic-machine-learning/</link>
      <pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/08/14/introduction-to-probabilistic-machine-learning/</guid>
      <description>&lt;p&gt;Incase you missed it, here is a recording of my talk on Introduction to Probabilisitic Machine Learning at the Las Vegas R &amp;amp; Data Science Meetup groups.&lt;/p&gt;
&lt;p&gt;I introduced probabilistic machine learning and probabilistic programming with Stan. I discussed the basics of machine learning from a probabilistic/Bayesian perspective and contrasted it with traditional/algorithmic machine learning. I also discussed how to build probabilistic models in computer code using a new exciting programming paradigm called Probabilistic Programming (PP). Particularly I used Stan (within R), a PP language, to build models ranging from simple generalized linear models to complex hierarchical models and nonparametric models for machine learning.&lt;/p&gt;
&lt;p&gt;Slides and code can be found on Github &lt;a href=&#34;https://bit.ly/intro-pml-lv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;part-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part I&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/ICFtztrK9a4&#34; frameborder=&#34;0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;div id=&#34;part-ii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part II&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/YsGAce_3Ql4&#34; frameborder=&#34;0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>In case you missed it: My Webinar on Model-Based Machine Learning</title>
      <link>/post/2016/08/03/in-case-you-missed-it-my-webinar-on-model-based-machine-learning/</link>
      <pubDate>Wed, 03 Aug 2016 07:41:44 +0000</pubDate>
      
      <guid>/post/2016/08/03/in-case-you-missed-it-my-webinar-on-model-based-machine-learning/</guid>
      <description>&lt;p&gt;In case you missed my free webinar on &amp;ldquo;&lt;strong&gt;&lt;a href=&#34;https://danielemaasit.com/post/2016/07/16/webinar-model-based-machine-learning-and-probabilistic-programming-using-rstan/&#34; target=&#34;_blank&#34;&gt;Model-Based Machine Learning&lt;/a&gt;&lt;/strong&gt;&amp;rdquo;,  here is the recording.&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//player.vimeo.com/video/175956118&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;


&lt;p&gt;Apologies for the poor quality of the video. Domino Data Lab&amp;rsquo;s webinar platform suffered a service degradation while recording the event. The webinar slides may be found below.&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/joTxMMvOmslHWt&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;

&lt;p&gt;If you have any questions, please do not hesitate to contact me. Finally, I would like to thank &lt;a href=&#34;https://www.linkedin.com/in/enthoven&#34; target=&#34;_blank&#34;&gt;Daniel Enthoven&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/danielchalef&#34; target=&#34;_blank&#34;&gt;Daniel Chalef&lt;/a&gt; from Domino Data Lab for setting up this webinar.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Gallery of ggplot2 Extensions</title>
      <link>/post/2016/07/29/a-gallery-of-ggplot2-extensions/</link>
      <pubDate>Fri, 29 Jul 2016 02:58:44 +0000</pubDate>
      
      <guid>/post/2016/07/29/a-gallery-of-ggplot2-extensions/</guid>
      <description>&lt;p&gt;A couple of months ago, I announced the &lt;a href=&#34;http://www.ggplot2-exts.org/&#34; target=&#34;_blank&#34;&gt;ggplot2-extensions website&lt;/a&gt; which tracks and lists extensions built on top of the popular R visualization package &lt;strong&gt;&lt;a href=&#34;http://docs.ggplot2.org/current/&#34; target=&#34;_blank&#34;&gt;ggplot2&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Now, I wanted to make it even easier for R users to filter and search for these extensions and so I have added a &lt;a href=&#34;http://www.ggplot2-exts.org/gallery/&#34; target=&#34;_blank&#34;&gt;Gallery page&lt;/a&gt;. You can now search packages based on a filter like: if it&amp;rsquo;s on CRAN; or if  it&amp;rsquo;s for a particular task e.g. time series, networks, tech, etc.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/gallery.png&#34; alt=&#34;gallery&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s now easier for R developers to add their extensions to this Gallery. Submit a pull request by following these &lt;a href=&#34;https://github.com/ggplot2-exts/gallery#adding-a-ggplot2-extension&#34; target=&#34;_blank&#34;&gt;simple instructions&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Fork the gallery &lt;a href=&#34;https://github.com/ggplot2-exts/gallery&#34; target=&#34;_blank&#34;&gt;repository&lt;/a&gt; on Github.&lt;/li&gt;
&lt;li&gt;Create a png thumbnail of an interesting plot from your extension that will look good on a retina screen at 350x300 pixels and put this file in the &lt;code&gt;images&lt;/code&gt; directory of &lt;a href=&#34;https://github.com/ggplot2-exts/gallery&#34; target=&#34;_blank&#34;&gt;the gallery repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Add an entry for your extension in the &lt;code&gt;_config.yml&lt;/code&gt; file of &lt;a href=&#34;https://github.com/ggplot2-exts/gallery&#34; target=&#34;_blank&#34;&gt;the repository&lt;/a&gt; with the meta data for your extension.&lt;/li&gt;
&lt;li&gt;Push your changes and create a pull request.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Acknowledgement&lt;/strong&gt;: Special thanks to &lt;a href=&#34;http://ryanhafen.com/&#34; target=&#34;_blank&#34;&gt;Dr. Ryan Hafen&lt;/a&gt; (&lt;a href=&#34;https://twitter.com/hafenstats&#34; target=&#34;_blank&#34;&gt;@hafenstats&lt;/a&gt;) for inspiring the design of this &lt;a href=&#34;http://www.ggplot2-exts.org/gallery/&#34; target=&#34;_blank&#34;&gt;Gallery page&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Webinar: Model-Based Machine Learning and Probabilistic Programming using RStan</title>
      <link>/post/2016/07/16/webinar-model-based-machine-learning-and-probabilistic-programming-using-rstan/</link>
      <pubDate>Sat, 16 Jul 2016 22:36:00 +0000</pubDate>
      
      <guid>/post/2016/07/16/webinar-model-based-machine-learning-and-probabilistic-programming-using-rstan/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;/img/mbml-webinar2.png&#34; alt=&#34;mbml-webinar2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I am glad to announce that I shall be presenting a live webinar with &lt;a href=&#34;https://www.dominodatalab.com/&#34; target=&#34;_blank&#34;&gt;Domino Data Labs&lt;/a&gt; on July 20, 2016 from 11:00 - 11:30 AM PST on &lt;a href=&#34;https://blog.dominodatalab.com/an-introduction-to-model-based-machine-learning/&#34; target=&#34;_blank&#34;&gt;Model-Based Machine Learning and Probabilistic Programming using RStan&lt;/a&gt;. If you are interested in adopting machine learning but are overwhelmed by the vast amount of learning algorithms, this webinar will show how to overcome that challenge. This &lt;a href=&#34;https://blog.dominodatalab.com/an-introduction-to-model-based-machine-learning/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; describes most of the material we will cover in the webinar. Here is the &lt;a href=&#34;https://www.dominodatalab.com/resource/webinar/model-based-machine-learning&#34; target=&#34;_blank&#34;&gt;abstract&lt;/a&gt; for the webinar:&lt;/p&gt;

&lt;h2 id=&#34;synopsis&#34;&gt;Synopsis&lt;/h2&gt;

&lt;p&gt;In the last several decades, thousands of machine learning algorithms have been developed. Very often, the selection of an algorithm to solve a particular problem is driven more by the data scientist&amp;rsquo;s familiarity with a small subset of available algorithms, than optimizing for predictive power or operational constraints. This is unsurprising: Newcomers to machine learning and veteran data scientists alike, may be overwhelmed by the multitude of machine learning algorithms and where and how it is most appropriate to use them.&lt;/p&gt;

&lt;p&gt;In this webinar, Daniel Emaasit will introduce Model-Based Machine Learning (MBML), an approach to machine learning which addresses these challenges. Daniel will discuss the various uses of MBML, from tasks such as classification, to regression and clustering, and how it allows data scientists to address the uncretainty inherent to real-world machine learning applications. Daniel will demonstrate how to implement MBML in a probabilistic programming language called Stan, using the RStan package. At the end of webinar, attendees will have the knowledge to build their own custom probabilistic models, learning their parameters from data.&lt;/p&gt;

&lt;p&gt;Click this &lt;a href=&#34;https://www.dominodatalab.com/resource/webinar/model-based-machine-learning&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to register for the webinar. I look forward to seeing you there and answering your questions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Incase you missed it: My Talk at the United Nations Global Pulse Workshop</title>
      <link>/post/2016/07/07/incase-you-missed-it-my-talk-at-the-united-nations-global-pulse-worshop/</link>
      <pubDate>Thu, 07 Jul 2016 23:16:47 +0000</pubDate>
      
      <guid>/post/2016/07/07/incase-you-missed-it-my-talk-at-the-united-nations-global-pulse-worshop/</guid>
      <description>&lt;p&gt;In case you missed my talk at the &lt;a href=&#34;http://www.datascienceafrica.org/dsa2016/#workshop&#34; target=&#34;_blank&#34;&gt;2016 Data Science Africa Workshop&lt;/a&gt; organized by the &lt;a href=&#34;http://unglobalpulse.org/kampala&#34; target=&#34;_blank&#34;&gt;United Nations Global Pulse Lab&lt;/a&gt;, here is the recording. My talk was titled &amp;ldquo;&lt;em&gt;&lt;a href=&#34;https://danielemaasit.com/post/2016/06/28/a-preview-of-my-talk-for-the-data-science-africa-workshop-organized-by-the-united-nations/&#34; target=&#34;_blank&#34;&gt;Sustainable Urban Transport Planning using Big Data from Mobile Phones&lt;/a&gt;&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/47IjdD2yyGE&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;You can download slides for my talk from &lt;a href=&#34;https://www.dropbox.com/s/v53ymxth8x4hpe1/dsa_2016_presentation.pdf?dl=0&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/6RhjZngF83Q7r9&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; 

&lt;p&gt;There were also talks from my colleagues at &lt;a href=&#34;http://www.research.ibm.com/labs/africa/&#34; target=&#34;_blank&#34;&gt;IBM Research - Africa&lt;/a&gt; including Oliver Bent, Simone Fobi and Skyler Speakman who gave a general overview of the work we are doing in our lab in Nairobi, Kenya. Here is their recording:&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/gxFstDWf9VU&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;I would like to thank Dr. John Quinn and the entire team a the &lt;a href=&#34;http://unglobalpulse.org/kampala&#34; target=&#34;_blank&#34;&gt;UNGP - Kampala&lt;/a&gt; for the invitation to speak and for a well organized event.  You can read more about the workshop from this &lt;a href=&#34;http://unglobalpulse.org/news/data-science-in-africa-2016&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; at the &lt;a href=&#34;http://unglobalpulse.org/blog&#34; target=&#34;_blank&#34;&gt;UNGP blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It was great meeting a lot of people from academia, industry, non-profits and government who are using Data Science to solve several challenges on the African continent, ranging from health care to agriculture and to sustainable cities. I look forward to attending more events in the future.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/dsa-pic.jpg&#34; alt=&#34;dsa-pic&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Preview of My Talk for the Data Science Africa Workshop organized by the United Nations</title>
      <link>/post/2016/06/28/a-preview-of-my-talk-for-the-data-science-africa-workshop-organized-by-the-united-nations/</link>
      <pubDate>Tue, 28 Jun 2016 10:36:32 +0000</pubDate>
      
      <guid>/post/2016/06/28/a-preview-of-my-talk-for-the-data-science-africa-workshop-organized-by-the-united-nations/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;/img/un.png&#34; alt=&#34;un&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I am excited to be invited by the &lt;a href=&#34;http://www.unglobalpulse.org/kampala&#34; target=&#34;_blank&#34;&gt;United Nations Global Pulse lab&lt;/a&gt; to speak at the &lt;a href=&#34;http://www.datascienceafrica.org/dsa2016/#workshop&#34; target=&#34;_blank&#34;&gt;2nd Data Science Africa Workshop&lt;/a&gt; scheduled to take place in Kampala, Uganda from 30th June to 1st July. The theme of this workshop is &amp;ldquo;&lt;em&gt;Using data science to monitor and achieve the global goals (UNDP goals) in Africa&lt;/em&gt;&amp;rdquo;. I will be speaking particularly on &amp;ldquo;&lt;em&gt;Data Science for Sustainable Cities&amp;rdquo;&lt;/em&gt;. My talk is titled: &amp;ldquo;&lt;em&gt;Sustainable Urban Transport Planning using Big Data from Mobile Phones&lt;/em&gt;&amp;rdquo;; which is the work I am doing as part of my PhD research. Particularly, I will talk about how developing countries can leverage low-cost, readily available and massive amounts of mobile phone data to improve their Transportation Planning policies.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/congestion1.jpg&#34; alt=&#34;congestion1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the past decades, there has been rapid urbanization as more and more people migrate into cities. The World Health Organization (WHO) estimates that by 2017, a majority of people will be living in urban areas. By 2030, 5 billion people—60 percent of the world’s population—will live in cities, compared with 3.6 billion in 2013. Developing nations must cope with this rapid urbanization. Transportation and urban planners must estimate travel demand for transportation facilities and use this to plan transportation infrastructure. Presently, the technique used for transportation planning uses data inputs from local and national household travel surveys. However, these surveys are expensive to conduct, cover smaller areas of cities and the time between surveys range from 5 to 10 years. This calls for new and innovative ways for Transportation Planning using new data sources.&lt;/p&gt;

&lt;p&gt;In recent years, we have witnessed the proliferation of ubiquitous mobile computing devices in developing countries. These mobile phones capture the movement of vehicles and people in near real time and generate massive amounts of new data.  My PhD research investigates how we can utilize anonymized mobile phone data ( i.e. Call Detail Records) and probabilistic machine learning to infer travel/mobility patterns. One of the objectives of this research is to demonstrate that these new “big” data sources are cheaper alternatives for transport modeling and travel behavior studies.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll have various UN and government people doing urban planning who I think would enjoy this topic — see you there! .&lt;/p&gt;

&lt;p&gt;If you haven’t already, register for &lt;a href=&#34;http://goo.gl/forms/Et8ztKOQmo&#34; target=&#34;_blank&#34;&gt;Data Science Africa Workshop 2016 here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Incase you missed it: My Webinar on Spatial Data Analysis with R</title>
      <link>/post/2016/02/25/incase-you-missed-it-my-webinar-on-spatial-data-analysis-with-r/</link>
      <pubDate>Thu, 25 Feb 2016 17:22:11 +0000</pubDate>
      
      <guid>/post/2016/02/25/incase-you-missed-it-my-webinar-on-spatial-data-analysis-with-r/</guid>
      <description>&lt;p&gt;In case you missed my free webinar on &amp;ldquo;&lt;strong&gt;&lt;a href=&#34;https://danielemaasit.com/post/2016/02/15/webinar-getting-started-with-spatial-data-analysis-with-r/&#34; target=&#34;_blank&#34;&gt;Getting Started with Spatial Data Analysis with R&lt;/a&gt;&lt;/strong&gt;&amp;rdquo;,  here is the recording.&lt;/p&gt;

&lt;iframe src=&#34;https://player.vimeo.com/video/156607218&#34; width=&#34;640&#34; height=&#34;480&#34; frameborder=&#34;0&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;You can access the material used for this webinar from &lt;a href=&#34;https://www.dominodatalab.com/&#34; target=&#34;_blank&#34;&gt;Domino Data Lab&lt;/a&gt;&amp;rsquo;s platform using the following links:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The Slides [&lt;a href=&#34;https://app.dominodatalab.com/SparkIQLabs/spatial-analysis/view/domino-presentation.pdf?commitId=fca395c2d9501d3282b24029fea7d16e6d8b91d0&#34; target=&#34;_blank&#34;&gt;domino-presentation.pdf&lt;/a&gt;]&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The RMarkdown Script [&lt;a href=&#34;https://app.dominodatalab.com/SparkIQLabs/spatial-analysis/view/ReadMe.Rmd?commitId=fca395c2d9501d3282b24029fea7d16e6d8b91d0&#34; target=&#34;_blank&#34;&gt;ReadMe.Rmd&lt;/a&gt;]&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://app.dominodatalab.com/SparkIQLabs/spatial-analysis/browse&#34; target=&#34;_blank&#34;&gt;The Whole Project&lt;/a&gt; [All files including data]&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you have any questions, please do not hesitate to contact me. If you have more topics (related to R) that you are interested in learning about, send them my way so we can prepare another webinar.&lt;/p&gt;

&lt;p&gt;Finally, I would like to thank &lt;a href=&#34;https://www.linkedin.com/in/annaanisin&#34; target=&#34;_blank&#34;&gt;Anna Anisin&lt;/a&gt; from Domino Data Lab for setting up this webinar.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Webinar: Getting Started with Spatial Data Analysis with R</title>
      <link>/post/2016/02/15/webinar-getting-started-with-spatial-data-analysis-with-r/</link>
      <pubDate>Mon, 15 Feb 2016 17:39:49 +0000</pubDate>
      
      <guid>/post/2016/02/15/webinar-getting-started-with-spatial-data-analysis-with-r/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;/img/webinar3.png&#34; alt=&#34;webinar3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I am glad to announce that I shall be presenting a live webinar with &lt;a href=&#34;https://www.dominodatalab.com/&#34; target=&#34;_blank&#34;&gt;Domino Data Labs&lt;/a&gt; on February 24, 2016 from 11:00 - 11:30 AM PST: &lt;a href=&#34;https://www.dominodatalab.com/resource/webinar/getting_started_spacial_data_analysis_r&#34; target=&#34;_blank&#34;&gt;Getting Started with Spatial Data Analysis with R&lt;/a&gt;. If you are interested or know someone interested in learning how to manipulate spatial and spatial-temporal data with R, please send them along. Here is the abstract:&lt;/p&gt;

&lt;h2 id=&#34;synopsis&#34;&gt;Synopsis&lt;/h2&gt;

&lt;p&gt;Spatial and spatial-temporal data have become pervasive. We are generating spatial data from route planners, sensors, mobile devices, and computers in different fields like transportation, agriculture, and social media. These data need to be analyzed to generate hidden insights that can improve business processes, help fight crime in cities, and much more.&lt;/p&gt;

&lt;p&gt;Simply creating static maps is not enough. In this webinar we shall look at techniques of importing and exporting spatial data into R; understanding the foundation classes for spatial data; manipulation of spatial data; and techniques for spatial visualization. This webinar is meant to provide introductory knowledge of spatial data analysis in R needed to understand more complex spatial data modeling techniques.&lt;/p&gt;

&lt;p&gt;We will cover the following topics:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Why use R for spatial analysis&lt;/li&gt;
&lt;li&gt;Packages for spatial data analysis&lt;/li&gt;
&lt;li&gt;Types of spatial data&lt;/li&gt;
&lt;li&gt;Classes and methods in R for spatial data analysis&lt;/li&gt;
&lt;li&gt;Importing and exporting spatial data&lt;/li&gt;
&lt;li&gt;Visualizing spatial data in R&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Click this &lt;a href=&#34;https://www.dominodatalab.com/resource/webinar/getting_started_spacial_data_analysis_r&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to register for the webinar. I look forward to seeing you there and answering your questions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tracking ggplot2 Extensions</title>
      <link>/post/2016/02/01/tracking-ggplot2-extensions/</link>
      <pubDate>Mon, 01 Feb 2016 23:55:06 +0000</pubDate>
      
      <guid>/post/2016/02/01/tracking-ggplot2-extensions/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;/img/ggplot2-2.png&#34; alt=&#34;ggplot2-2&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The purpose of this blog post is to inform R users of a &lt;a href=&#34;http://ggplot2-exts.github.io/index.html&#34; target=&#34;_blank&#34;&gt;website&lt;/a&gt; that I created to track and list &lt;strong&gt;ggplot2 extensions&lt;/strong&gt;. The site is available at: &lt;a href=&#34;http://ggplot2-exts.github.io&#34; target=&#34;_blank&#34;&gt;http://ggplot2-exts.github.io&lt;/a&gt;. The purpose of this site is to help other R users easily find &lt;a href=&#34;http://ggplot2.org/&#34; target=&#34;_blank&#34;&gt;ggplot2&lt;/a&gt; extensions that are coming in &amp;ldquo;fast and furious&amp;rdquo; from the R community.&lt;/p&gt;

&lt;p&gt;If you have developed or intend on developing ggplot2 extensions, submit them so that other R users can easily find them. To do so, simply create an issue or a pull request on this &lt;a href=&#34;https://github.com/ggplot2-exts/ggplot2-exts.github.io/tree/dev&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt; repository.&lt;/p&gt;

&lt;p&gt;Incase you are wondering what ggplot2 extensions are; these are R packages that extend the functionality of the &lt;a href=&#34;http://ggplot2.org/&#34; target=&#34;_blank&#34;&gt;ggplot2 package&lt;/a&gt; by &lt;a href=&#34;http://had.co.nz/&#34; target=&#34;_blank&#34;&gt;Dr. Hadley Wickham&lt;/a&gt;. This extensibility mechanism became available in ggplot2 version 2.0.0 &lt;a href=&#34;http://blog.rstudio.org/2015/12/21/ggplot2-2-0-0/&#34; target=&#34;_blank&#34;&gt;released on December 17th, 2015&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;motivation-for-developing-this-website&#34;&gt;&lt;strong&gt;Motivation for Developing this Website&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;When Hadley announced the release of ggplot2 2.0.0, perhaps the most exciting news was the addition of an official extension mechanism. This mechanism allows other R developers to easily create their on stats, geoms and positions, and provide them in other packages. This means that even when less development occurs in the ggplot2 package itself, the community will continue to release packages for graphical analysis that extend/solve different requirements. (To learn how you can develop your own extensions, there&amp;rsquo;s a step by step tutorial here: &lt;a href=&#34;https://cran.r-project.org/web/packages/ggplot2/vignettes/extending-ggplot2.html&#34; target=&#34;_blank&#34;&gt;https://cran.r-project.org/web/packages/ggplot2/vignettes/extending-ggplot2.html&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Immediately after this announcement, several extensions started popping up. I was able to learn of some because they were posted out on twitter by &lt;a href=&#34;https://twitter.com/hadleywickham&#34; target=&#34;_blank&#34;&gt;@hadleywickham&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/screenshot_2016-01-30-10-52-46-1.png&#34; alt=&#34;Screenshot_2016-01-30-10-52-46-1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;But it got me thinking&amp;hellip;.. R users who are not active on social media must be missing out on these new cool extensions. In addition, if I didn&amp;rsquo;t check my twitter feed on a particular day, I would miss tweets about new extension-packages. This was not an effective way of searching for packages. There has to be a better way to track and list ggplot2 extensions. Quickly, I sprang into action to help fill this gap. But first, I had to find out if such an initiative already existed in the R community.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/screenshot_2016-01-30-10-52-27-1.png&#34; alt=&#34;Screenshot_2016-01-30-10-52-27-1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It turned out none existed. I quickly started developing one and a couple of hours later, we had a working website (&lt;a href=&#34;http://ggplot2-exts.github.io/&#34; target=&#34;_blank&#34;&gt;http://ggplot2-exts.github.io&lt;/a&gt;) with a list of extensions known to me then.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/ggplot2-3.png&#34; alt=&#34;ggplot2-3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hadley was kind enough to inform his followers on twitter about this new initiative. (I should mention, it was the highlight of my life getting a &amp;ldquo;thumps up&amp;rdquo; from Hadley Wickham).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/screenshot_2016-02-01-15-12-16-1.png&#34; alt=&#34;Screenshot_2016-02-01-15-12-16-1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As it turns out, I was not the only one concerned about this issue. Several other R users had made similar inquiries.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/screenshot_2016-01-30-10-49-14-2.png&#34; alt=&#34;Screenshot_2016-01-30-10-49-14-2&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;closing-remarks&#34;&gt;&lt;strong&gt;Closing Remarks&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In closing, I hope to see more cool extensions being developed and shared with the rest of the community on &lt;a href=&#34;http://ggplot2-exts.github.io&#34; target=&#34;_blank&#34;&gt;ggplot2-exts.github.io&lt;/a&gt; so that other R Users can easily find them. Don&amp;rsquo;t forget to create an issue or a pull request on this &lt;a href=&#34;https://github.com/ggplot2-exts/ggplot2-exts.github.io/tree/dev&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt; repo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Apache SparkR to Power Shiny Applications: Part I</title>
      <link>/post/2015/12/08/using-apache-sparkr-to-power-shiny-applications-part-i/</link>
      <pubDate>Tue, 08 Dec 2015 17:24:26 +0000</pubDate>
      
      <guid>/post/2015/12/08/using-apache-sparkr-to-power-shiny-applications-part-i/</guid>
      <description>

&lt;h3 id=&#34;shiny-sparkr-img-shiny-sparkr-jpg&#34;&gt;&lt;img src=&#34;/img/shiny-sparkr.jpg&#34; alt=&#34;shiny-sparkr&#34; /&gt;&lt;/h3&gt;

&lt;h3 id=&#34;introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The objective of this blog post is demonstrate how to use &lt;a href=&#34;http://spark.apache.org&#34; target=&#34;_blank&#34;&gt;Apache SparkR&lt;/a&gt; to power &lt;a href=&#34;http://shiny.rstudio.com&#34; target=&#34;_blank&#34;&gt;Shiny applications&lt;/a&gt;. I have been curious about what the use cases for a &amp;ldquo;Shiny-SparkR&amp;rdquo; application would be and how to develop and deploy such an app.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SparkR&lt;/strong&gt; is an R package that provides a light-weight frontend to use Apache Spark from R. SparkR provides a distributed data frame implementation that supports operations like selection, filtering, aggregation etc. (similar to R data frames, dplyr) but on large datasets. SparkR also supports distributed machine learning using MLlib.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Shiny&lt;/strong&gt; is an open source R package that provides an elegant and powerful web framework for building web applications using R. Shiny helps you turn your analyses into interactive web applications without requiring HTML, CSS, or JavaScript knowledge.&lt;/p&gt;

&lt;h3 id=&#34;use-cases&#34;&gt;&lt;strong&gt;Use Cases&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;So you&amp;rsquo;re probably asking yourself, &amp;ldquo;Why would I need to use SparkR to run my Shiny applications?&amp;rdquo;. That is a legitimate question and to answer it, we need to understand the different classes of big data problems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Classes of Big Data Problems&lt;/strong&gt;
In a recent &lt;a href=&#34;http://bit.ly/1LbWPhl&#34; target=&#34;_blank&#34;&gt;AMA on Reddit&lt;/a&gt;, &lt;a href=&#34;http://had.co.nz/&#34; target=&#34;_blank&#34;&gt;Hadley Wickham&lt;/a&gt; (Chief Scientist at &lt;a href=&#34;https://www.rstudio.com/&#34; target=&#34;_blank&#34;&gt;RStudio&lt;/a&gt;) painted a clearer picture of how &amp;ldquo;Big Data&amp;rdquo; should be defined. His insights will help us to define uses cases for SparkR and Shiny.&lt;/p&gt;

&lt;p&gt;I believe big data problems should be categorized in 3 main classes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Big Data-Small Analytics:&lt;/strong&gt; This is where a data scientist begins with a raw big dataset and then slices and dices that data to obtain the right sample required to answer a specific business/research problem. In most cases the resulting sample is a small dataset, which &lt;strong&gt;doesnot&lt;/strong&gt; require the use of SparkR to run a shiny application.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Partition Aggregrate Analytics:&lt;/strong&gt; This is where a data scientist needs to distribute and parallelize computation over multiple machines. Wickham defines this problem as a &lt;strong&gt;trivially parallelisable problem&lt;/strong&gt;. An example is when you need to fit one model per individual for thousands of individuals. In this case SparkR is a good fit but there are also packages in R that solve this problem such as the &lt;a href=&#34;https://cran.r-project.org/web/packages/foreach/index.html&#34; target=&#34;_blank&#34;&gt;foreach package&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Big Data-Large Scale Analytics&lt;/strong&gt;. This is where a data scientist needs all the big data, perhaps because they are fitting a complex model. An example of this type of problem is recommender systems which really do benefit from lots of data because they need to recognize interactions that occur only rarely. SparkR is a perfect fit for this problem when developing Shiny applications.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Memory Considerations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Also, it&amp;rsquo;s important to take into consideration memory availability and size when looking into such an application. This can be viewed in two different ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you are running your shiny applications on servers that have more than enough memory to fit your big data, then you probrably do not need SparkR. Nowadays there is accessibility to machines with terabytes on RAM from cloud providers like &lt;a href=&#34;http://aws.amazon.com&#34; target=&#34;_blank&#34;&gt;Amazon AWS&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If your big data cannot fit on one machine, you may need to distribute it on several machines. SparkR is a perfect fit for this problem because it provides distributed algorithms that can crunch your data on different worker nodes and return the result to the master node.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;a-simple-illustrative-example&#34;&gt;&lt;strong&gt;A Simple Illustrative Example&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Before we start understanding how each piece of such an application would operate, let&amp;rsquo;s download and run this simple Shiny-SparkR application. Go to this github repository &lt;a href=&#34;https://github.com/SparkIQ-Labs/Demos&#34; target=&#34;_blank&#34;&gt;https://github.com/SparkIQ-Labs/Demos&lt;/a&gt; and access the &lt;strong&gt;&amp;ldquo;shiny-sparkr-demo-1&amp;rdquo;&lt;/strong&gt; example.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/SparkIQ-Labs/Demos/blob/master/shiny-sparkr-demo-1/img/repo.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://github.com/SparkIQ-Labs/Demos/raw/master/shiny-sparkr-demo-1/img/repo.png&#34; alt=&#34;Repository&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Make sure you already have Apache Spark 1.5 or later downloaded onto your computer. Instructions for downloading and starting SparkR can be found in this &lt;a href=&#34;http://bit.ly/1kP5Fbm&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Make sure you have Java 1.7.x installed and the environment variables are set.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;launch-the-app&#34;&gt;&lt;strong&gt;Launch the App&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Once you have downloaded the app-folder, open the project in RStudio and open the &lt;strong&gt;&amp;ldquo;server.R&amp;rdquo;&lt;/strong&gt; file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. **Change Spark Home**. Change the path of the **SPARK_HOME** environment variable to point to the destination of your Spark installation.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/SparkIQ-Labs/Demos/raw/master/shiny-sparkr-demo-1/img/spark-home.png&#34; alt=&#34;Change Spark home&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Run the App&lt;/strong&gt;. Run the shiny app by using this command &lt;code&gt;shiny::runApp()&lt;/code&gt;. It will take some time for SparkR to be initialized before you can see the results of the underlying analysis are displayed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/SparkIQ-Labs/Demos/raw/master/shiny-sparkr-demo-1/img/app.png&#34; alt=&#34;The App&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here is the the code for the &amp;ldquo;server.R&amp;rdquo; file.&lt;/p&gt;

&lt;p&gt;[gist]7cf8aa8efc535db160df[/gist]&lt;/p&gt;

&lt;h3 id=&#34;what-happens-underneath&#34;&gt;&lt;strong&gt; What happens Underneath.&lt;/strong&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;1. **Stage 1:** When you run the app, the user interface is displayed but without the rendered text output or model summary.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/SparkIQ-Labs/Demos/raw/master/shiny-sparkr-demo-1/img/no-results.png&#34; alt=&#34;App without the results&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. **Stage 2:** Meanwhile, in the background on your computer node(s), java is launched using the Spark-submit file, then the SparkR library is loaded and then SparkR is initialized.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/SparkIQ-Labs/Demos/raw/master/shiny-sparkr-demo-1/img/java-launch.png&#34; alt=&#34;SparkR is initialized&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. **Stage 3:** SparkR commands in the Server.R file are then executed, which finally shows the output within the shiny app.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/SparkIQ-Labs/Demos/raw/master/shiny-sparkr-demo-1/img/app.png&#34; alt=&#34;Results in the App&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can use the Spark UI to check the jobs that were completed, in the event timeline, to produce the final results in the shiny app. Go to localhost and listen on port 4040.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/SparkIQ-Labs/Demos/raw/master/shiny-sparkr-demo-1/img/event-timeline.png&#34; alt=&#34;Results in the App&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Stage 4:&lt;/strong&gt; When you change the input values in the app and click the &amp;ldquo;Predict Sepal Length&amp;rdquo; button, the application uses the already exciting Spark Context to run the predict function and displays the predicted value. This operations takes a shorter time than the initial launch of the shiny app.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/SparkIQ-Labs/Demos/raw/master/shiny-sparkr-demo-1/img/new-result.png&#34; alt=&#34;Change values&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;moving-forward&#34;&gt;&lt;strong&gt;Moving Forward&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The objective of this first demo was to learn the use cases for SparkR and Shiny; and to see what happens underneath when you eventually deploy and run such an application on a PC.&lt;/p&gt;

&lt;p&gt;In &lt;strong&gt;Part II&lt;/strong&gt; of this tutorial series, we shall see how to develop and deploy such an application for a &amp;ldquo;Big Data-Large Scale Analytics&amp;rdquo; problem on big data stored on a cluster on AWS EC2. As we have already established this is one of the perfect use cases for SparkR and Shiny.&lt;/p&gt;

&lt;p&gt;Please share your thoughts and experiences in the comments&amp;rsquo; section below if you have built such applications.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Launch Apache Spark on AWS EC2 and Initialize SparkR Using RStudio</title>
      <link>/post/2015/11/10/launch-apache-spark-on-aws-ec2-and-initialize-sparkr-using-rstudio/</link>
      <pubDate>Tue, 10 Nov 2015 09:41:40 +0000</pubDate>
      
      <guid>/post/2015/11/10/launch-apache-spark-on-aws-ec2-and-initialize-sparkr-using-rstudio/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/sparkr-ec2.jpg&#34; alt=&#34;sparkr-ec2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this blog post, we shall learn how to launch a Spark stand alone cluster on &lt;a href=&#34;http://aws.amazon.com/&#34; target=&#34;_blank&#34;&gt;Amazon Web Services (AWS) Elastic Compute Cloud (EC2)&lt;/a&gt; for analysis of Big Data. This is a continuation from our &lt;a href=&#34;https://danielemaasit.com/post/2015/07/26/installing-and-starting-sparkr-locally-on-windows-os-and-rstudio/&#34; target=&#34;_blank&#34;&gt;previous blog&lt;/a&gt;, which showed us how to download &lt;a href=&#34;http://spark.apache.org/&#34; target=&#34;_blank&#34;&gt;Apache Spark&lt;/a&gt; and start SparkR locally on windows OS and &lt;a href=&#34;https://www.rstudio.com/&#34; target=&#34;_blank&#34;&gt;RStudio&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We shall use &lt;em&gt;Spark 1.5.1&lt;/em&gt; (released on October 02, 2015) which has a &lt;em&gt;spark-ec2&lt;/em&gt; script that is used to install stand alone Spark on AWS EC2.  A nice feature about this &lt;em&gt;spark-ec2&lt;/em&gt; script is that it installs RStudio server as well. This means that you don&amp;rsquo;t need to install RStudio server separately. Thus you can start working with your data immediately after Spark is installed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You should have already downloaded &lt;a href=&#34;http://spark.apache.org/&#34; target=&#34;_blank&#34;&gt;Apache Spark&lt;/a&gt; onto your local desktop from the &lt;a href=&#34;http://spark.apache.org/&#34; target=&#34;_blank&#34;&gt;official site&lt;/a&gt;. You can find instructions on how to do so in our &lt;a href=&#34;http://blog.sparkiq-labs.com/2015/07/26/installing-and-starting-sparkr-locally-on-windows-os-and-rstudio/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You should have an AWS account, created secret access key(s) and downloaded your private key pair as a .pem file. Find instructions on how to create your access keys &lt;a href=&#34;http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and to download your private keys &lt;a href=&#34;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We will launch the clusters through &lt;a href=&#34;http://www.gnu.org/software/bash/manual/bashref.html&#34; target=&#34;_blank&#34;&gt;Bash shell&lt;/a&gt; on Linux. If you are using Windows OS I recommend that you install and use the &lt;a href=&#34;http://www.cygwin.com/&#34; target=&#34;_blank&#34;&gt;Cygwin&lt;/a&gt; terminal (It provides functionality similar to a Linux distribution on Windows)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Launching Apache Spark on AWS EC2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We shall use the &lt;em&gt;spark-ec2&lt;/em&gt; script, located in Spark&amp;rsquo;s &lt;em&gt;ec2&lt;/em&gt; directory to launch, manage and shutdown Spark clusters on Amazon EC2. It will setup Spark, HDFS, Tachyon, RStudio on your cluster.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1: Go into the ec2 directory&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Change directory into the &amp;ldquo;&lt;em&gt;ec2&amp;rdquo;&lt;/em&gt; directory. In my case, I downloaded Spark onto my desktop, so I ran this command.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ cd Desktop/Apache/spark-1.5.1/ec2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/1-cd.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/1-cd.png?w=300&#34; alt=&#34;1-cd&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2: Set environment variables&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Set the environment variables &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt; to your Amazon EC2 access key ID and secret access key.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ export AWS_SECRET_ACCESS_KEY=AaBbCcDdEeFGgHhIiJjKkLlMmNnOoPpQqRrSsTtU&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ export AWS_ACCESS_KEY_ID=ABCDEFG1234567890123&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3: Launch the spark-ec2 script&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Launch the cluster by running the following command.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ ./spark-ec2 --key-pair=awskey --identity-file=awskey.pem --region=us-east-1 --instance-type=c3.4xlarge -s 2 --copy-aws-credentials launch test-cluster&lt;/code&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/2-launch.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/2-launch.png?w=300&#34; alt=&#34;2-launch&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Where;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&amp;ndash;key-pair=&lt;name_of_your_key_pair&gt; , The name of your EC2 key pair&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&amp;ndash;identity-file=&lt;name_of_your_key_pair&gt;.pem , The private key file&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&amp;ndash;region=&lt;the_region_where_key_pair_was_created&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&amp;ndash;instance-type=&lt;the_instance_you_want&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;-s N, where N is the number of slave nodes&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&amp;ldquo;test-cluster&amp;rdquo; is the name of the cluster&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In case you want to set other options for the launch of your cluster, further instructions can be found on the &lt;a href=&#34;http://spark.apache.org/docs/latest/ec2-scripts.html&#34; target=&#34;_blank&#34;&gt;Spark documentation website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As I mentioned earlier, this script also installs RStudio server, as can be seen in the figure below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/2-launch-awsscreen.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/3-install-rstudio.png?w=300&#34; alt=&#34;3-install-rstudio&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The cluster installation takes about 7 minutes. When it is done, the host address of the master node is displayed at the end of the log message as shown in the figure below. At this point your Spark cluster has been installed successfully and you are a ready to start exploring and analyzing your data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/4-done.png?w=300&#34; alt=&#34;4-done&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Before you continue, you may be curious to see whether your cluster is actually up and running. Simply log into your AWS account and go to the EC2 dashboard. In my case, I have 1 master node and 2 slave/worker nodes in my Spark cluster.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/2-launch-awsscreen.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/2-launch-awsscreen.png?w=300&#34; alt=&#34;2-launch-awsScreen&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Use the address displayed at the end of the launch message and access the Spark User Interface (UI) on port 8080. You can also get the host address of your master node by using the &amp;ldquo;&lt;em&gt;get-master&lt;/em&gt;&amp;rdquo; option in the command below.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ ./spark-ec2 --key-pair=awskey --identity-file=awskey.pem get-master test-cluster&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/5-online.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/5-online.png?w=300&#34; alt=&#34;5-online&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 4: Login to your cluster&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the terminal you can login to your master node by using the &amp;ldquo;&lt;em&gt;login&lt;/em&gt;&amp;rdquo; option in the following command&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ ./spark-ec2 --key-pair=awskey --identity-file=awskey.pem login test-cluster&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/6-login.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/6-login.png?w=300&#34; alt=&#34;6-login&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 5 (Optional): Start the SparkR REPL&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here you can actually start the SparkR REPL by typing the following command.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ spark/bin/sparkR&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/7-start-sparkr.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/7-start-sparkr.png?w=300&#34; alt=&#34;7-start-sparkr&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;SparkR will be initialized and you should see a welcome message as shown in the Figure below. Here you can actually start working with your data. However most R users, like myself, would like to work in an Integrated Development Environment (IDE) like RStudio. See steps 6 and 7 on how to do so.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/8-welcome-sparkr.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/8-welcome-sparkr.png?w=300&#34; alt=&#34;8-welcome-sparkr&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 6: Create user accounts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Use the following command to list all available users on the cluster.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ cut -d: -f1 /etc/passwd&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/9-users.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/9-users.png?w=300&#34; alt=&#34;9-users&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You will notice that &amp;ldquo;rstudio&amp;rdquo; is one of the available user accounts. You can create other user accounts and passwords for them using these commands.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ sudo adduser daniel&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ passwd daniel&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;In my case, I used the &amp;ldquo;rstudio&amp;rdquo; user account and changed its password.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/10-create-passwd.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/10-create-passwd.png?w=300&#34; alt=&#34;10-create-passwd&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Initializing SparkR Using RStudio&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;spark-ec2&lt;/em&gt; script also created a &amp;ldquo;&lt;em&gt;startSpark.R&lt;/em&gt;&amp;rdquo; script that we shall use to initialize SparkR.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 7: Login to RStudio server&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Using the username you selected/created and the password you created, login into RStudio server.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/11-rstudio.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/11-rstudio.png?w=300&#34; alt=&#34;11-rstudio&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 8: Initialize SparkR&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you log in to RStudio server, you will see the &amp;rdquo;startSpark.R&amp;rdquo; in your files pane (already created for you).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/12-startsparkr.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/12-startsparkr.png?w=300&#34; alt=&#34;12-startSparkR&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Simply run the &amp;ldquo;startSpark.R&amp;rdquo; script to initialize SparkR. This creates a Spark Context and a SQL Context for you.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/13-initialize.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/13-initialize.png?w=300&#34; alt=&#34;13-initialize&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 9: Start Working with your Data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now you are ready to start working with your data.&lt;/p&gt;

&lt;p&gt;Here I use a simple example of the &amp;ldquo;mtcars&amp;rdquo; dataset to show that you can now run SparkR commands and use the MLLib library to run a simple linear regression model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/14-lm-example.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/14-lm-example.png?w=300&#34; alt=&#34;14-lm-example&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can view the status of your jobs by using the host address of your master and listening on port 4040. This UI also displays a chain of RDD dependencies organized in Direct Acyclic Graph (DAG) as shown in the figure below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/15-dag.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://sparkiqlabs.files.wordpress.com/2015/11/15-dag.png?w=222&#34; alt=&#34;15-DAG&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Final Remarks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The objective of this blog post was to show you how to get started with Spark on AWS EC2 and initialize SparkR using RStudio. In the next blog post we shall look into working with actual &amp;ldquo;Big&amp;rdquo; datasets stored in different data stores such as &lt;a href=&#34;https://aws.amazon.com/s3/&#34; target=&#34;_blank&#34;&gt;Amazon S3&lt;/a&gt; or &lt;a href=&#34;https://www.mongodb.com/&#34; target=&#34;_blank&#34;&gt;MongoDB&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Further Interests: RStudio Shiny + SparkR&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I am curious about how to use &lt;a href=&#34;http://shiny.rstudio.com/&#34; target=&#34;_blank&#34;&gt;Shiny&lt;/a&gt; with SparkR and in the next couple of days I will investigate this idea further. The question is: how can one use SparkR to power shiny applications. If you have any thoughts please share them in the comments section below and let&amp;rsquo;s discuss.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
