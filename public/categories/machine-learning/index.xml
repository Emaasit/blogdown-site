<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Daniel Emaasit</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on Daniel Emaasit</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Daniel Emaasit</copyright>
    <lastBuildDate>Mon, 07 Aug 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/machine-learning/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Gaussian Process models in Stan</title>
      <link>/post/2017/gaussian-process-models-in-stan/</link>
      <pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/gaussian-process-models-in-stan/</guid>
      <description>&lt;p&gt;In this post we present RStan, the R interface to Stan. Stan is a C++ library for Bayesian inference using the No-U-Turn sampler (a variant of Hamiltonian Monte Carlo) or frequentist inference via optimization. We illustrate the features of RStan through an example in &lt;span class=&#34;citation&#34;&gt;@GelmanCarlinSternRubin:2003&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Stan is a C++ library for Bayesian modeling and inference that primarily uses the No-U-Turn sampler (NUTS) &lt;span class=&#34;citation&#34;&gt;[@hoffman-gelman:2012]&lt;/span&gt; to obtain posterior simulations given a user-specified model and data. Alternatively, Stan can utilize the LBFGS optimization algorithm to maximize an objective function, such as a log-likelihood. The R package &lt;strong&gt;rstan&lt;/strong&gt; provides RStan, the R interface to Stan. The &lt;strong&gt;rstan&lt;/strong&gt; package allows one to conveniently fit Stan models from R &lt;span class=&#34;citation&#34;&gt;[@rprj]&lt;/span&gt; and access the output, including posterior inferences and intermediate quantities such as evaluations of the log posterior density and its gradients.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Probabilistic Machine Learning</title>
      <link>/post/2017/introduction-to-probabilistic-machine-learning/</link>
      <pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/introduction-to-probabilistic-machine-learning/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
