<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Daniel Emaasit on Daniel Emaasit</title>
    <link>http://www.danielemaasit.com/</link>
    <description>Recent content in Daniel Emaasit on Daniel Emaasit</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Daniel Emaasit</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Gaussian Processes with Spectral Mixture Kernels to Implicitly Capture Hidden Structure from Data</title>
      <link>http://www.danielemaasit.com/post/2018/03/19/gaussian-processes-with-spectral-mixture-kernels-to-implicitly-capture-hidden-structure-from-data/</link>
      <pubDate>Mon, 19 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/post/2018/03/19/gaussian-processes-with-spectral-mixture-kernels-to-implicitly-capture-hidden-structure-from-data/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;(Note: Cross-posted with the &lt;a href=&#34;https://wp.me/p7rVtH-1Fv&#34; target=&#34;_blank&#34;&gt;Haystax Technology Blog&lt;/a&gt;.)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Several scientific fields such as insider-threat detection, highway-safety planning, often lack sufficient amounts of time-series training data for the purpose of scientific discovery. Moreover, the available limited data are quite noisy. For instance Greitzer and Ferryman (2013) state that ”ground truth” data on actual insider behavior is typically either not available or is limited. In some cases, one might acquire real data, but for privacy reasons, there is no attribution of any individuals relating to abuses or offenses i.e., there is no ground truth. The data may contain insider threats, but these are not identified or knowable to the researcher (Greitzer and Ferryman, 2013; Gheyas and Abdallah, 2016).In highway-safety
planning, Veeramisti (2016) mentions that Departments of Transportation (DOTs) only recently started collecting monthly highway-crash data because of the high cost and extensive process of collecting the required data.&lt;/p&gt;

&lt;h1 id=&#34;the-problem&#34;&gt;The Problem&lt;/h1&gt;

&lt;p&gt;Having limited and quite noisy data for insider-threat detection and highway-safety planning presents a major challenge when estimating time-series models that are robust to overfitting and have well-calibrated uncertainty estimates. Most of the current literature in time-series modeling in these scientific fields is associated with two major limitations.&lt;/p&gt;

&lt;p&gt;First, the methods involve visualizing the time series for noticeable structure and patterns such as periodicity, smoothness, growing/decreasing trends and then hard-coding these patterns into the statistical models during formulation. This approach is suitable for large datasets where more data typically provides more information to learn expressive structure. Given limited amounts of data, such expressive structure may not be easily noticeable. For instance, the figure below shows monthly attachment size in emails (in Gigabytes) sent by an insider from their employee account to their home account. Trends such as periodicity, smoothness, growing/decreasing trends are not easily noticeable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/data-emails.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Second, most of the current literature focuses on parametric models that impose strong restrictive assumptions by pre-specifying the functional form and number of parameters. Pre-specifying a functional form for a time-series model could lead to either overly complex model specifications or simplistic models. It is difficult to know &lt;em&gt;a priori&lt;/em&gt; the most appropriate function to use for modeling sophisticated insider-threat behavior or highway-crash scenarios that involve complex hidden patterns and many other influencing factors.&lt;/p&gt;

&lt;h2 id=&#34;source-code&#34;&gt;Source code&lt;/h2&gt;

&lt;p&gt;For the impatient reader, two options are provided below to access the source code used for empirical analyses:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The entire project (code, notebooks, data, and results) can be found &lt;a href=&#34;https://github.com/Emaasit/long-range-extrapolation&#34; target=&#34;_blank&#34;&gt;here on GitHub&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Click the binder icon below to open the notebooks in a web browser and explore the entire project without downloading and installing any software.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/Emaasit/long-range-extrapolation/master?urlpath=lab&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg&#34; alt=&#34;Binder&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;data-science-questions&#34;&gt;Data Science Questions&lt;/h1&gt;

&lt;p&gt;Given the above limitations in the current state-of-art, this study formulated the following three Data Science questions. Given limited and quite noisy time-series data for insider-threat detection and highway-safety planning, is it possible to perform:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;pattern discovery without hard-coding trends into statistical models during formulation?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;model estimation that precludes pre-specifying a functional form?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;model estimation that is robust to overfitting and has well-calibrated uncertainty estimates?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;hypothesis&#34;&gt;Hypothesis&lt;/h1&gt;

&lt;p&gt;To answer these three Data Science questions and address the above-described limitations, this study formulated the following hypothesis:
&lt;blockquote&gt;This study hypothesizes that by leveraging current state-of-the-art innovations in nonparametric Bayesian methods, such as Gaussian processes with spectral mixture kernels, it is possible to perform pattern discovery without prespecifying functional forms and hard-coding trends into statistical models.&lt;/blockquote&gt;&lt;/p&gt;

&lt;h1 id=&#34;methodology&#34;&gt;Methodology&lt;/h1&gt;

&lt;p&gt;To test the above hypothesis, a nonparametric Bayesian approach was proposed to implicitly capture hidden structure from time series having limited data. The proposed model, a Gaussian process with a spectral mixture kernel, precludes the need to pre-specify a functional form and hard code trends, is robust to overfitting and has well-calibrated uncertainty estimates.&lt;/p&gt;

&lt;p&gt;Mathematical details of the proposed model formulation are described in a corresponding paper that can be found on arXiv through the link below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Emaasit, D. and Johnson, M. (2018). &lt;a href=&#34;https://arxiv.org/abs/1803.05867&#34; target=&#34;_blank&#34;&gt;Capturing Structure Implicitly from Noisy Time-Series having Limited Data&lt;/a&gt;. arXiv preprint arXiv:1803.05867.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A Brief description of the fundamental concepts of the proposed methodology is as follows. Consider for each data point, $i$, that $y_i$ represents the attachment size in emails sent by an insider to their home account and $x_i$ is a temporal covariate such as month. The task is to estimate a latent function $f$, which maps input data, $x_i$, to output data $y_i$ for $i$ = 1, 2, $\ldots{}$, $N$, where $N$ is the total number of data points. Each of the input data $x_i$ is of a single dimension $D = 1$, and $\textbf{X}$ is a $N$ x $D$ matrix with rows $x_i$.&lt;/p&gt;

&lt;p&gt;&lt;img class=&#34;size-medium wp-image-6429 aligncenter&#34; src=&#34;http://haystax.com/wp-content/uploads/2018/03/gp-pgm-352x300.png&#34; alt=&#34;&#34; width=&#34;352&#34; height=&#34;200&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The observations are assumed to satisfy:&lt;/p&gt;

&lt;p&gt;\begin{equation}
y_i = f(x&lt;em&gt;i) + \varepsilon, \quad where \, \, \varepsilon \sim \mathcal{N}(0, \sigma&lt;/em&gt;{\varepsilon}^2)
\end{equation}&lt;/p&gt;

&lt;p&gt;The noise term, $\varepsilon$, is assumed to be normally distributed with a zero mean and variance, $\sigma_{\varepsilon}^2$. Latent function $f$ represents hidden underlying trends that produced the observed time-series data.&lt;/p&gt;

&lt;p&gt;Given that it is difficult to know $\textit{a priori}$ the most appropriate functional form to use for $f$, a prior distribution, $p(\textbf{f})$, over an infinite number of possible functions of interest is formulated. A natural prior over an infinite space of functions is a Gaussian process prior (Williams and Rasmussen, 2006). A GP is fully parameterized by a mean function, $\textbf{m}$, and covariance function, $\textbf{K}_{N,N}$, denoted as:&lt;/p&gt;

&lt;p&gt;\begin{equation}\label{eqn:gpsim}
\textbf{f} \sim \mathcal{GP}(\textbf{m}, \textbf{K}_{N,N}),
\end{equation}&lt;/p&gt;

&lt;p&gt;The posterior distribution over the unknown function evaluations, $\textbf{f}$, at all data points, $x_i$, was estimated using Bayes theorem as follows:&lt;/p&gt;

&lt;p&gt;\begin{equation}\label{eqn:bayesinfty}
\begin{aligned}
p(\textbf{f} \mid \textbf{y},\textbf{X}) = \frac{p(\textbf{y} \mid \textbf{f}, \textbf{X}) \, p(\textbf{f})}{p(\textbf{y} \mid \textbf{X})} &lt;br /&gt;
= \frac{p(\textbf{y} \mid \textbf{f}, \textbf{X}) \, \mathcal{N}(\textbf{f} \mid \textbf{m}, \textbf{K}_{N,N})}{p(\textbf{y} \mid \textbf{X})},
\end{aligned}
\end{equation}&lt;/p&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;p&gt;\begin{aligned}
p(\textbf{f}\mid \textbf{y},\textbf{X}) = \text{the posterior distribution of functions that best explain the response variable, given the covariates}
\end{aligned}&lt;/p&gt;

&lt;p&gt;\begin{aligned}
p(\textbf{y} \mid \textbf{f}, \textbf{X}) = \text{the likelihood of response variable, given the functions and covariates}&lt;br /&gt;
\end{aligned}&lt;/p&gt;

&lt;p&gt;\begin{aligned}
p(\textbf{f}) = \text{the prior over all possible functions of the response variable}
\end{aligned}&lt;/p&gt;

&lt;p&gt;\begin{aligned}
p(\textbf{y} \mid \textbf{X}) = \text{the data (constant)}&lt;br /&gt;
\end{aligned}&lt;/p&gt;

&lt;p&gt;A spectral mixture kernel was proposed for the covariance function, $\textbf{K}_{N,N}$. The resulting posterior, $p(\textbf{f}\mid \textbf{y},\textbf{X})$ is a Gaussian process composed of a distribution of possible functions that best explain the time-series pattern.&lt;/p&gt;

&lt;h1 id=&#34;experiments&#34;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&#34;the-setup&#34;&gt;The setup&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s first install some python packages that we shall use for our analysis. Also we shall set up our plotting requirements.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_context(&#39;notebook&#39;, font_scale = 1.1)
np.random.seed(12345)
rc = {&#39;xtick.labelsize&#39;: 40, &#39;ytick.labelsize&#39;: 40, &#39;axes.labelsize&#39;: 40, &#39;font.size&#39;: 40, &#39;lines.linewidth&#39;: 4.0, 
      &#39;lines.markersize&#39;: 40, &#39;font.family&#39;: &amp;quot;serif&amp;quot;, &#39;font.serif&#39;: &amp;quot;cm&amp;quot;, &#39;savefig.dpi&#39;: 200,
      &#39;text.usetex&#39;: False, &#39;legend.fontsize&#39;: 40.0, &#39;axes.titlesize&#39;: 40, &amp;quot;figure.figsize&amp;quot;: [24, 16]}
sns.set(rc = rc)
sns.set_style(&amp;quot;darkgrid&amp;quot;)
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = &amp;quot;all&amp;quot;
import gpflow
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;raw-data-and-sample-formation&#34;&gt;Raw data and sample formation&lt;/h2&gt;

&lt;p&gt;The insider-threat data used for empirical analysis in this study was provided by the computer emergency response team (CERT) division of the software engineering institute (SEI) at Carnegie Mellon University. The particular insider threat focused on is the case where a known insider sent information as email attachments from their work email to their home email.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s read in the data using &lt;code&gt;pandas&lt;/code&gt;, view the first three records and the structure of the resulting &lt;code&gt;pandas&lt;/code&gt; dataframe.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;email_filtered = pd.read_csv(&amp;quot;../data/emails/email_filtered.csv&amp;quot;, parse_dates=[&amp;quot;date&amp;quot;])
email_filtered.head(n = 3)
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;user&lt;/th&gt;
      &lt;th&gt;pc&lt;/th&gt;
      &lt;th&gt;to&lt;/th&gt;
      &lt;th&gt;cc&lt;/th&gt;
      &lt;th&gt;bcc&lt;/th&gt;
      &lt;th&gt;from&lt;/th&gt;
      &lt;th&gt;activity&lt;/th&gt;
      &lt;th&gt;size&lt;/th&gt;
      &lt;th&gt;attachments&lt;/th&gt;
      &lt;th&gt;content&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;{D0V4-N9KM15BF-0512LLVP}&lt;/td&gt;
      &lt;td&gt;2010-01-04 07:36:48&lt;/td&gt;
      &lt;td&gt;BTR2026&lt;/td&gt;
      &lt;td&gt;PC-9562&lt;/td&gt;
      &lt;td&gt;Thaddeus.Brett.Daniel@dtaa.com&lt;/td&gt;
      &lt;td&gt;Zorita.Angela.Wilson@dtaa.com&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Beau.Todd.Romero@dtaa.com&lt;/td&gt;
      &lt;td&gt;Send&lt;/td&gt;
      &lt;td&gt;23179&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;On November 25, general Savary was sent to the...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;{L5E5-J1HB80OY-9539AOEC}&lt;/td&gt;
      &lt;td&gt;2010-01-04 07:38:18&lt;/td&gt;
      &lt;td&gt;BTR2026&lt;/td&gt;
      &lt;td&gt;PC-9562&lt;/td&gt;
      &lt;td&gt;Beau.Todd.Romero@dtaa.com&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Marsh_Travis@raytheon.com&lt;/td&gt;
      &lt;td&gt;View&lt;/td&gt;
      &lt;td&gt;17047&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Early in the morning of May 27, a boat crossed...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;{Q4V7-V6BR00TZ-5209UVDX}&lt;/td&gt;
      &lt;td&gt;2010-01-04 07:53:35&lt;/td&gt;
      &lt;td&gt;BTR2026&lt;/td&gt;
      &lt;td&gt;PC-9562&lt;/td&gt;
      &lt;td&gt;Bianca-Clark@optonline.net&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Beau_Romero@aol.com&lt;/td&gt;
      &lt;td&gt;Send&lt;/td&gt;
      &lt;td&gt;26507&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;The Americans never held up their side of the ...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Our &lt;code&gt;pandas&lt;/code&gt; dataframe comprises columns that we are interested in such as &amp;ldquo;user&amp;rdquo; (username), &amp;ldquo;date&amp;rdquo;, &amp;ldquo;to&amp;rdquo; (the recipient email address) and &amp;ldquo;size&amp;rdquo; (attachment size) of emails.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;email_filtered.info()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 11920 entries, 0 to 11919
Data columns (total 12 columns):
id             11920 non-null object
date           11920 non-null datetime64[ns]
user           11920 non-null object
pc             11920 non-null object
to             11920 non-null object
cc             6101 non-null object
bcc            593 non-null object
from           11920 non-null object
activity       11920 non-null object
size           11920 non-null int64
attachments    3809 non-null object
content        11920 non-null object
dtypes: datetime64[ns](1), int64(1), object(10)
memory usage: 1.1+ MB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s filter data for a particular known insider with user ID &amp;ldquo;CDE1846&amp;rdquo; and summarize the total attachment size of emails by month. This results into 17 data points ranging from January 2010 to May 2011.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_insider = email_filtered[email_filtered[&amp;quot;user&amp;quot;] == &amp;quot;CDE1846&amp;quot;]
emails_per_month = df_insider.resample(rule = &amp;quot;1M&amp;quot;, on = &amp;quot;date&amp;quot;).sum().reset_index()
emails_per_month[&amp;quot;date&amp;quot;] = pd.to_datetime(emails_per_month[&amp;quot;date&amp;quot;], format = &amp;quot;%Y-%m-%d&amp;quot;)
emails_per_month.columns = [&amp;quot;ds&amp;quot;, &amp;quot;y&amp;quot;]
emails_per_month.y = emails_per_month.y/1e6
emails_per_month[&amp;quot;ds&amp;quot;] = emails_per_month[&amp;quot;ds&amp;quot;].apply(lambda x: x.strftime(&#39;%Y-%m&#39;)).astype(str)
emails_per_month
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style&gt;
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ds&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2010-01&lt;/td&gt;
      &lt;td&gt;117.809274&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2010-02&lt;/td&gt;
      &lt;td&gt;112.461320&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2010-03&lt;/td&gt;
      &lt;td&gt;134.592245&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;2010-04&lt;/td&gt;
      &lt;td&gt;148.911866&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2010-05&lt;/td&gt;
      &lt;td&gt;80.689085&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;2010-06&lt;/td&gt;
      &lt;td&gt;128.024029&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;2010-07&lt;/td&gt;
      &lt;td&gt;115.046041&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;2010-08&lt;/td&gt;
      &lt;td&gt;142.607937&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;2010-09&lt;/td&gt;
      &lt;td&gt;121.728119&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;2010-10&lt;/td&gt;
      &lt;td&gt;126.041467&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;2010-11&lt;/td&gt;
      &lt;td&gt;80.338345&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;2010-12&lt;/td&gt;
      &lt;td&gt;113.142879&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;2011-01&lt;/td&gt;
      &lt;td&gt;95.485553&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;2011-02&lt;/td&gt;
      &lt;td&gt;88.279993&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;2011-03&lt;/td&gt;
      &lt;td&gt;148.193802&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;2011-04&lt;/td&gt;
      &lt;td&gt;221.337135&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;2011-05&lt;/td&gt;
      &lt;td&gt;146.220533&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Let&amp;rsquo;s visualize this data using &lt;code&gt;matplotlib&lt;/code&gt; and &lt;code&gt;seaborn&lt;/code&gt;. The resulting figure shows no interesting insights just yet.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.barplot(data = emails_per_month, x = &amp;quot;ds&amp;quot;, y = &amp;quot;y&amp;quot;, color = &amp;quot;blue&amp;quot;, saturation = .5)
ax.set_xticklabels(labels = emails_per_month.ds, rotation = 45)
ax.set_xlabel(&#39;Time&#39;)
ax.set_ylabel(&#39;Total size of emails in GB&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/output_12_0.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s look at the case where the insider sent email IP from their employee account to their home account. Visualizing this data shows some interesting trends towards the end of the analysis period. The attachment size increases drastically in March and April of 2011.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df_insider_non_org = df_insider[~df_insider[&#39;to&#39;].str.contains(&#39;dtaa.com&#39;)]
df_insider_ewing = df_insider_non_org[df_insider_non_org[&#39;to&#39;] == &#39;Ewing_Carlos@comcast.net&#39;]
df = df_insider_ewing.resample(&#39;1M&#39;, on=&#39;date&#39;).sum().reset_index()
df.columns = [&amp;quot;ds&amp;quot;, &amp;quot;y&amp;quot;]
df.y = df.y/1e6
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&amp;quot;ds&amp;quot;] = df[&amp;quot;ds&amp;quot;].apply(lambda x: x.strftime(&#39;%Y-%m&#39;)).astype(str)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
sns.barplot(data = df, x = &amp;quot;ds&amp;quot;, y = &amp;quot;y&amp;quot;, color = &amp;quot;blue&amp;quot;, saturation = .5)
ax.set_xticklabels(labels = df.ds, rotation = 45)
ax.set_xlabel(&#39;Time&#39;)
ax.set_ylabel(&#39;Total size of emails in GB&#39;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/output_16_0.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;empirical-analysis&#34;&gt;Empirical analysis&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s drop the anormalous data points from our dataframe so that we can train a model for the normal behaviour and then create a training dataset of size = 11 and remaining data our for testing the estimated model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.drop([14, 15, 16])
test_size = 11
X_complete = np.array([df.index]).reshape((df.shape[0], 1)).astype(&#39;float64&#39;)
X_train = X_complete[0:test_size, ]
X_test = X_complete[test_size:df.shape[0], ]
Y_complete = np.array([df.y]).reshape((df.shape[0], 1)).astype(&#39;float64&#39;)
Y_train = Y_complete[0:test_size, ]
Y_test = Y_complete[test_size:df.shape[0], ]
D = Y_train.shape[1];
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots()
ax.plot(X_train.flatten(),Y_train.flatten(), c =&#39;k&#39;, marker = &amp;quot;o&amp;quot;, label = &amp;quot;Training data&amp;quot;)
ax.plot(X_test.flatten(),Y_test.flatten(), c=&#39;b&#39;, marker = &amp;quot;o&amp;quot;, label = &#39;Test data&#39;)
ax.set_xticks(ticks = df.index)
ax.set_xticklabels(labels = df.ds, rotation = 45)
ax.set_xlabel(&#39;Time&#39;)
ax.set_ylabel(&#39;Total size of emails in GB&#39;)
plt.legend(loc = &amp;quot;best&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/output_20_0.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s now develop a Gaussian Process model with a Spectral Mixture (SM) kernel proposed by Wilson (2014). This is because the SM kernel is capable of capturing hidden structure with data without hard cording features in a kernel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Trains a model with a spectral mixture kernel, given an ndarray of 
# 2Q frequencies and lengthscales

Q = 10 # nr of terms in the sum
max_iters = 1000

def create_model(hypers):
    f = np.clip(hypers[:Q], 0, 5)
    weights = np.ones(Q) / Q
    lengths = hypers[Q:]

    kterms = []
    for i in range(Q):
        rbf = gpflow.kernels.RBF(D, lengthscales=lengths[i], variance=1./Q)
        rbf.lengthscales.transform = gpflow.transforms.Exp()
        cos = gpflow.kernels.Cosine(D, lengthscales=f[i])
        kterms.append(rbf * cos)

    k = np.sum(kterms) + gpflow.kernels.Linear(D) + gpflow.kernels.Bias(D)
    m = gpflow.gpr.GPR(X_train, Y_train, kern=k)
    return m

m = create_model(np.ones((2*Q,)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s perfrom inference through optimization of the likelihood.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%time
m.optimize(maxiter = max_iters)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPU times: user 7.74 s, sys: 289 ms, total: 8.03 s
Wall time: 7.61 s


      fun: 20.868585670810997
 hess_inv: &amp;lt;43x43 LbfgsInvHessProduct with dtype=float64&amp;gt;
      jac: array([  8.99958679e-06,   1.41339465e-05,   5.09060783e-05,
         6.72588106e-06,  -1.28315446e-08,   1.22652879e-05,
         5.09060783e-05,   6.72588106e-06,  -1.28315446e-08,
         1.22652879e-05,   5.09060783e-05,   6.72588106e-06,
        -1.28315446e-08,   1.22652879e-05,   5.09060783e-05,
         6.72588106e-06,  -1.28315446e-08,   1.22652879e-05,
         5.09060783e-05,   6.72588106e-06,  -1.28315446e-08,
         1.22652879e-05,   5.09060783e-05,   6.72588106e-06,
        -1.28315446e-08,   1.22652879e-05,   5.09060783e-05,
         6.72588106e-06,  -1.28315446e-08,   1.22652879e-05,
         5.09060783e-05,   6.72588106e-06,  -1.28315446e-08,
         1.22652879e-05,   5.09060783e-05,   6.72588106e-06,
        -1.28315446e-08,   1.22652879e-05,   5.09060783e-05,
         6.72588106e-06,  -1.28315446e-08,   1.22652879e-05,
         5.10663145e-06])
  message: b&#39;CONVERGENCE: REL_REDUCTION_OF_F_&amp;lt;=_FACTR*EPSMCH&#39;
     nfev: 50
      nit: 42
   status: 0
  success: True
        x: array([  2.1321322 ,  -1.88610378,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.63568472,   1.38389724,
        10.77485046,  -1.51730421,   0.26608891])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plotprediction(m):
    # Perform prediction
    mu, var = m.predict_f(X_complete)

    # Plot
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.set_xticks(ticks = df.index)
    ax.set_xticklabels(labels = df.ds, rotation = 45)
    ax.set_xlabel(&#39;Time&#39;)
    ax.set_ylabel(&#39;Total size of emails in GB&#39;);
    ax.plot(X_train.flatten(),Y_train.flatten(), c=&#39;k&#39;, marker = &amp;quot;o&amp;quot;, label = &#39;Training data&#39;)
    ax.plot(X_test.flatten(),Y_test.flatten(), c=&#39;b&#39;, marker = &amp;quot;o&amp;quot;, label = &#39;Test data&#39;)
    ax.plot(X_complete.flatten(), mu.flatten(), c=&#39;g&#39;, marker = &amp;quot;o&amp;quot;, label = &amp;quot;Predicted mean function&amp;quot;)
    lower = mu - 2*np.sqrt(var)
    upper = mu + 2*np.sqrt(var)
    ax.plot(X_complete, upper, &#39;g--&#39;, X_complete, lower, &#39;g--&#39;, lw=1.2)
    ax.fill_between(X_complete.flatten(), lower.flatten(), upper.flatten(),
                    color=&#39;g&#39;, alpha=.1, label = &amp;quot;95% Predicted credible interval&amp;quot;)
    plt.legend(loc = &amp;quot;best&amp;quot;)
    plt.tight_layout()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plotprediction(m);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/output_26_0.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Figure above shows that the Gaussian process model with a spectral mixture kernel is able to capture the structure both in regions of the training and testing data. The 95% predicted credible interval (CI) contains the &amp;ldquo;normal&amp;rdquo; size of email attachments for the duration of the measurements.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s calculate some performance measures such as the Root Mean Square Error (RMSE) and Mean Absolute Performance Error (MAPE).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## Calculate the RMSE and MAPE
def calculate_rmse(model, X_test, Y_test):
    mu, var = model.predict_y(X_test)
    rmse = np.sqrt(((mu - Y_test)**2).mean())
    return rmse

def calculate_mape(model, X_test, Y_test):
    mu, var = model.predict_y(X_test)
    mape = (np.absolute(((mu - Y_test)/Y_test)*100)).mean()
    return mape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;calculate_rmse(model=m, X_test = X_test, Y_test = Y_test)
calculate_mape(model=m, X_test = X_test, Y_test = Y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1.2515806168637664
27.94536660649003
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s estimate an ARIMA model was estimated using the &lt;code&gt;statsmodels&lt;/code&gt; package for comparison.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import itertools
import numpy.ma as ma
import warnings
from statsmodels.tsa.arima_model import ARIMA
from numpy.linalg import LinAlgError


def get_ARIMA_param_values(y):
    &amp;quot;&amp;quot;&amp;quot; Get best ARIMA values given data
    &amp;quot;&amp;quot;&amp;quot;
    warnings.filterwarnings(&#39;ignore&#39;)
    
    # Values to try
    p = [0, 1, 2, 3, 4, 5, 6]
    d = [0, 1, 2]
    q = [0, 1, 2, 3, 4, 5, 6]
    results = []

    for pi, di, qi in itertools.product(p, d, q):
        try:
            model = ARIMA(y, order=(pi, di, qi))
            model_fit = model.fit()
            aic = model_fit.aic
            if not np.isnan(aic):
                results.append(((pi,di,qi), aic, model_fit))
        except ValueError:
            pass
        except LinAlgError:
            pass
    warnings.filterwarnings(&#39;default&#39;)
    return sorted(results, key=lambda x: x[1])[0]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make prediction
steps = X_test.shape[0]
params, aic, model_fit = get_ARIMA_param_values(y = Y_train)
mu, stderr, conf_int = model_fit.forecast(steps = steps, alpha=0.05)
params, aic, mu, stderr, conf_int
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;((0, 1, 0),
 47.811434155792767,
 array([ 6.475238,  7.047388,  7.619538]),
 array([ 2.16329641,  3.05936312,  3.7469393 ]),
 array([[  2.23525495,  10.71522105],
        [  1.05114646,  13.04362954],
        [  0.27567193,  14.96340407]]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def plotprediction_arima(m):
    mu, stderr, conf_int = m.forecast(steps=steps, alpha=0.05)

    # Plot
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.set_xticks(ticks = df.index)
    ax.set_xticklabels(labels = df.ds, rotation = 45)
    ax.set_xlabel(&amp;quot;Time&amp;quot;)
    ax.set_ylabel(&amp;quot;Total size of emails in GB&amp;quot;);
    ax.plot(X_train.flatten(), Y_train.flatten(), c=&#39;k&#39;, marker = &amp;quot;o&amp;quot;, label = &#39;Training data&#39;)
    ax.plot(X_test.flatten(), Y_test.flatten(), c=&#39;b&#39;, marker = &amp;quot;o&amp;quot;, label = &#39;Test data&#39;)
    ax.plot(X_test.flatten(), mu.flatten(), c=&#39;g&#39;, marker = &amp;quot;o&amp;quot;, label = &amp;quot;Predicted value&amp;quot;)
    lower = conf_int[:,0 ]
    upper = conf_int[:,1 ]
    ax.plot(X_test, upper, &#39;g--&#39;, X_test, lower, &#39;g--&#39;, lw=1.2)
    ax.fill_between(X_test.flatten(), lower.flatten(), upper.flatten(),
                    color=&#39;g&#39;, alpha=.1, label = &amp;quot;95% confidence interval&amp;quot;)
    plt.legend(loc = &amp;quot;best&amp;quot;)
    plt.tight_layout()

plotprediction_arima(m = model_fit);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://github.com/Emaasit/long-range-extrapolation/blob/dev/blog/output_34_0.png?raw=true&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Figure above shows that the ARIMA model is poor at capturing the structure within the region of testing data. This finding suggests that ARIMA models have poor performance for small data without noticeable structure. The 95% confidence interval for ARIMA is much wider than the GP model showing a high degree of uncertainty about the ARIMA predictions.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s calculate some performance measures such as the RMSE and MAPE for the ARIMA model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;## Calculate the RMSE and MAPE
def calculate_rmse_arima(mu, Y_test):
    rmse = np.sqrt(((mu - Y_test)**2).mean())
    return rmse

def calculate_mape_arima(mu, Y_test):
    mape = (np.absolute(((mu - Y_test)/Y_test)*100)).mean()
    return mape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;calculate_rmse_arima(mu = mu, Y_test = Y_test)
calculate_mape_arima(mu = mu, Y_test = Y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;3.2019469508164868
93.44165723388285
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The ARIMA model has a poor predictive performance compared to the Gaussian process model with a spectral mixture kernel&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;This study proposed a Bayesian nonparametric framework to capture implicitly hidden structure in time-series having limited data. The proposed framework, a Gaussian process with a spectral mixture kernel, was applied to time-series process for insider-threat data. The proposed framework addresses two current challenges when analyzing quite noisy time-series having limited data whereby the time series are visualized for noticeable structure such as periodicity, growing or decreasing trends and hard coding them into pre-specified functional forms. Experiments demonstrated that results from this framework outperform traditional ARIMA when the time series does not have easily noticeable structure and is quite noisy. Future work will involve evaluating the proposed framework on other different types of insider-threat behavior.&lt;/p&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Emaasit, D. and Johnson, M. (2018). Capturing Structure Implicitly from Noisy Time-Series having Limited Data. arXiv preprint arXiv:1803.05867.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Williams, C. K. and Rasmussen, C. E. (2006). Gaussian processes for machine learning. the MIT Press, 2(3):4.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Knudde, N., van der Herten, J., Dhaene, T., &amp;amp; Couckuyt, I. (2017). GPflowOpt: A Bayesian Optimization Library using TensorFlow. arXiv preprint arXiv:1711.03845.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Wilson, A. G. (2014). Covariance kernels for fast automatic pattern discovery and extrapolation with Gaussian processes. University of Cambridge.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Greitzer, F. L. and Ferryman, T. A. (2013). Methods and metrics for evaluating analytic insider threat tools. In Security and Privacy Workshops (SPW), 2013 IEEE, pages 90–97. IEEE.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gheyas, I. A. and Abdallah, A. E. (2016). Detection and prediction of insider threats to cybersecurity: a systematic literature review and meta-analysis. Big Data Analytics, 1(1):6.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Veeramisti, N. K. (2016). A business intelligence framework for network-level traffic safety analyses. PhD thesis, University of Nevada, Las Vegas.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;computing-environment&#34;&gt;Computing Environment&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# print system information/setup
%reload_ext watermark
%watermark -v -m -p numpy,pandas,gpflowopt,gpflow,tensorflow,matplotlib,ipywidgets,seaborn -g
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CPython 3.6.3
IPython 6.2.1

numpy 1.13.3
pandas 0.20.3
gpflowopt 0.1.0
gpflow 0.4.0
tensorflow 1.4.1
matplotlib 2.1.1
ipywidgets 7.1.1
seaborn 0.8.1

compiler   : GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)
system     : Darwin
release    : 17.3.0
machine    : x86_64
processor  : i386
CPU cores  : 8
interpreter: 64bit
Git hash   : df453b6da183c9f8fd941eaa1696e68f9731c771
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ICYMI: Probabilistic Programming Roundup November 2017</title>
      <link>http://www.danielemaasit.com/post/2017/12/04/icymi-probabilistic-programming-roundup-november-2017/</link>
      <pubDate>Mon, 04 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/post/2017/12/04/icymi-probabilistic-programming-roundup-november-2017/</guid>
      <description>&lt;p&gt;In case you missed them, here are some articles from November 2017 of particular interest to users of &lt;a href=&#34;http://probabilistic-programming.org&#34;&gt;Probabilistic Programming Languages&lt;/a&gt;(PPL).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://uber.ai/&#34;&gt;Uber AI&lt;/a&gt; Lab &lt;a href=&#34;https://eng.uber.com/pyro/&#34;&gt;open sourced their deep probabilistic programming language&lt;/a&gt; called &lt;a href=&#34;http://pyro.ai/&#34;&gt;Pyro&lt;/a&gt;. Pyro is a tool for deep probabilistic modeling, unifying the best of modern deeplearning and Bayesian modeling.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://buff.ly/2yQBDPu&#34;&gt;PyMC3 documentation&lt;/a&gt; was improved with new styling and builds on travis. You can help at this &lt;a href=&#34;https://buff.ly/2yQfilg&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Prof. Aki Vehtari prepared &lt;a href=&#34;http://mc-stan.org/users/documentation/case-studies/gpareto_functions.html&#34;&gt;this case study&lt;/a&gt; to demonstrate how to implement user defined probability functions in Stan.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Michael Betancourt updated his &lt;a href=&#34;https://t.co/4PwsEjjoTf&#34;&gt;personal page with several Stan case studies&lt;/a&gt;. Check out the robust Stan workflows in &lt;a href=&#34;https://t.co/iT2WHLNVcQ&#34;&gt;PyStan&lt;/a&gt; and &lt;a href=&#34;https://t.co/ilgsXyo5Pu&#34;&gt;RStan&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“Statistical Rethinking” book by Richard McElreath was &lt;a href=&#34;https://t.co/B4q3PYa8bJ&#34;&gt;ported to PyMC3 and Python&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A message by Christopher Fonnesbeck on &lt;a href=&#34;https://www.numfocus.org/blog/theano-and-the-future-of-pymc/S&#34;&gt;Theano and the Future of PyMC&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Join this &lt;a href=&#34;https://t.co/O467r31TF1&#34;&gt;discussion&lt;/a&gt; to help shape the future of PyMC3. With Theano development stopping, a new backend is needed for PyMC.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A &lt;a href=&#34;https://t.co/M4NaUrXErB&#34;&gt;study using PyMC3 about penalty scoring &amp;amp; saving ability&lt;/a&gt; in Major League Soccer (MLS). The conclusion was that it is hard to tell who is good at them.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In this video, Prof. Zoubin Ghahramani (Uber AI labs &amp;amp; University of Cambridge) made the case for the use of Probabilistic Programming. &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/-47G_ULKAHk?t=43m13s&#34; frameborder=&#34;0&#34; gesture=&#34;media&#34; allow=&#34;encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Blei et al. (2017) prepared this gentle &lt;a href=&#34;https://arxiv.org/abs/1601.00670&#34;&gt;review of variational inference&lt;/a&gt; for non-ML researchers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A &lt;a href=&#34;https://t.co/rFZ2iNOgSS&#34;&gt;lecture series&lt;/a&gt; to help you get started with Bayesian modeling using PyMC3.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://austinrochford.com/resources/talks/nba-fouls-pydata-nyc-2017.slides.html#/&#34;&gt;Slides&lt;/a&gt; by Austin Rochford explaining NBA foul calls with Python and PyMC3.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A blog post by Jim Savage on &lt;a href=&#34;https://t.co/1yt97FeTCo&#34;&gt;Bayesian Instrumental variables in Stan&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A Blog post about &lt;a href=&#34;https://t.co/1Te3gNAHlI&#34;&gt;diffusion/Wiener model analysis with Stan&lt;/a&gt; using R package brms.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kevin Pei’s blog post on &lt;a href=&#34;https://t.co/8AB7UkdUtj&#34;&gt;hierarchical Bayesian rating in PyMC3&lt;/a&gt; with application to eSports.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please send any suggestions that I may have missed to me at &lt;a href=&#34;mailto:daniel.emaasit@gmail.com&#34;&gt;daniel.emaasit@gmail.com&lt;/a&gt;. For weekly updates, follow me on Twitter @&lt;a href=&#34;https://twitter.com/emaasit&#34;&gt;emaasit&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Process Regression and Classification with Stan</title>
      <link>http://www.danielemaasit.com/talk/gaussian-processes-in-stan/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/talk/gaussian-processes-in-stan/</guid>
      <description>

&lt;h3 id=&#34;pre-requisites&#34;&gt;Pre-requisites:&lt;/h3&gt;

&lt;p&gt;The following software tools are required to run the demo(s):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;R + RStudio:- Follow this &lt;a href=&#34;https://www.r-project.org/&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to install R. Also install &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/preview/&#34; target=&#34;_blank&#34;&gt;RStudio&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;rstan:- Follow this &lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to install rstan.&lt;/li&gt;
&lt;li&gt;bayesplot:- Follow this &lt;a href=&#34;https://github.com/stan-dev/bayesplot&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to install bayesplot&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>An Integrated Travel Demand Model of Work Duration and Commute-mode Choices: A Flexible Modeling Approach</title>
      <link>http://www.danielemaasit.com/publication/integrated-tdm/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/publication/integrated-tdm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ICYMI: My talk on Introduction to Probabilistic Machine Learning</title>
      <link>http://www.danielemaasit.com/post/2017/08/14/introduction-to-probabilistic-machine-learning/</link>
      <pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/post/2017/08/14/introduction-to-probabilistic-machine-learning/</guid>
      <description>&lt;p&gt;Incase you missed it, here is a recording of my talk on Introduction to Probabilisitic Machine Learning at the Las Vegas R &amp;amp; Data Science Meetup groups.&lt;/p&gt;
&lt;p&gt;I introduced probabilistic machine learning and probabilistic programming with Stan. I discussed the basics of machine learning from a probabilistic/Bayesian perspective and contrasted it with traditional/algorithmic machine learning. I also discussed how to build probabilistic models in computer code using a new exciting programming paradigm called Probabilistic Programming (PP). Particularly I used Stan (within R), a PP language, to build models ranging from simple generalized linear models to complex hierarchical models and nonparametric models for machine learning.&lt;/p&gt;
&lt;p&gt;Slides and code can be found on Github &lt;a href=&#34;https://bit.ly/intro-pml-lv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;part-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part I&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/ICFtztrK9a4&#34; frameborder=&#34;0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;div id=&#34;part-ii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part II&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/YsGAce_3Ql4&#34; frameborder=&#34;0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Probabilistic Machine Learning with Stan</title>
      <link>http://www.danielemaasit.com/talk/intro-to-pml/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/talk/intro-to-pml/</guid>
      <description>

&lt;h3 id=&#34;pre-requisites&#34;&gt;Pre-requisites:&lt;/h3&gt;

&lt;p&gt;The following software tools are required to run the demo(s):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;R + RStudio:- Follow this &lt;a href=&#34;https://www.r-project.org/&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to install R. Also install &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/preview/&#34; target=&#34;_blank&#34;&gt;RStudio&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;rstan:- Follow this &lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to install rstan.&lt;/li&gt;
&lt;li&gt;bayesplot:- Follow this &lt;a href=&#34;https://github.com/stan-dev/bayesplot&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to install bayesplot&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Capturing Latent Travel Patterns with an Unknown Number of Activities</title>
      <link>http://www.danielemaasit.com/talk/capturing-latent-travel-patterns/</link>
      <pubDate>Thu, 12 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/talk/capturing-latent-travel-patterns/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The living roads project: Giving a voice to roads in developing cities</title>
      <link>http://www.danielemaasit.com/publication/living-roads-project/</link>
      <pubDate>Wed, 11 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/publication/living-roads-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Stochastic dynamic user equilibrium using a mixed logit modeling framework</title>
      <link>http://www.danielemaasit.com/publication/stochastic-dynamic-user-equilibrium-mixed-logit/</link>
      <pubDate>Mon, 26 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/publication/stochastic-dynamic-user-equilibrium-mixed-logit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mixed logit-based stochastic dynamic user equilibrium</title>
      <link>http://www.danielemaasit.com/publication/mixed-logic-based/</link>
      <pubDate>Sat, 01 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/publication/mixed-logic-based/</guid>
      <description></description>
    </item>
    
    <item>
      <title>In case you missed it: My Webinar on Model-Based Machine Learning</title>
      <link>http://www.danielemaasit.com/post/2016/08/03/in-case-you-missed-it-my-webinar-on-model-based-machine-learning/</link>
      <pubDate>Wed, 03 Aug 2016 07:41:44 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/post/2016/08/03/in-case-you-missed-it-my-webinar-on-model-based-machine-learning/</guid>
      <description>&lt;p&gt;In case you missed my free webinar on &amp;ldquo;&lt;strong&gt;&lt;a href=&#34;https://danielemaasit.com/post/2016/07/16/webinar-model-based-machine-learning-and-probabilistic-programming-using-rstan/&#34; target=&#34;_blank&#34;&gt;Model-Based Machine Learning&lt;/a&gt;&lt;/strong&gt;&amp;rdquo;,  here is the recording.&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//player.vimeo.com/video/175956118&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
 &lt;/div&gt;


&lt;p&gt;Apologies for the poor quality of the video. Domino Data Lab&amp;rsquo;s webinar platform suffered a service degradation while recording the event. The webinar slides may be found below.&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/joTxMMvOmslHWt&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;

&lt;p&gt;If you have any questions, please do not hesitate to contact me. Finally, I would like to thank &lt;a href=&#34;https://www.linkedin.com/in/enthoven&#34; target=&#34;_blank&#34;&gt;Daniel Enthoven&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/danielchalef&#34; target=&#34;_blank&#34;&gt;Daniel Chalef&lt;/a&gt; from Domino Data Lab for setting up this webinar.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Gallery of ggplot2 Extensions</title>
      <link>http://www.danielemaasit.com/post/2016/07/29/a-gallery-of-ggplot2-extensions/</link>
      <pubDate>Fri, 29 Jul 2016 02:58:44 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/post/2016/07/29/a-gallery-of-ggplot2-extensions/</guid>
      <description>&lt;p&gt;A couple of months ago, I announced the &lt;a href=&#34;http://www.ggplot2-exts.org/&#34; target=&#34;_blank&#34;&gt;ggplot2-extensions website&lt;/a&gt; which tracks and lists extensions built on top of the popular R visualization package &lt;strong&gt;&lt;a href=&#34;http://docs.ggplot2.org/current/&#34; target=&#34;_blank&#34;&gt;ggplot2&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Now, I wanted to make it even easier for R users to filter and search for these extensions and so I have added a &lt;a href=&#34;http://www.ggplot2-exts.org/gallery/&#34; target=&#34;_blank&#34;&gt;Gallery page&lt;/a&gt;. You can now search packages based on a filter like: if it&amp;rsquo;s on CRAN; or if  it&amp;rsquo;s for a particular task e.g. time series, networks, tech, etc.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.danielemaasit.com/img/gallery.png&#34; alt=&#34;gallery&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s now easier for R developers to add their extensions to this Gallery. Submit a pull request by following these &lt;a href=&#34;https://github.com/ggplot2-exts/gallery#adding-a-ggplot2-extension&#34; target=&#34;_blank&#34;&gt;simple instructions&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Fork the gallery &lt;a href=&#34;https://github.com/ggplot2-exts/gallery&#34; target=&#34;_blank&#34;&gt;repository&lt;/a&gt; on Github.&lt;/li&gt;
&lt;li&gt;Create a png thumbnail of an interesting plot from your extension that will look good on a retina screen at 350x300 pixels and put this file in the &lt;code&gt;images&lt;/code&gt; directory of &lt;a href=&#34;https://github.com/ggplot2-exts/gallery&#34; target=&#34;_blank&#34;&gt;the gallery repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Add an entry for your extension in the &lt;code&gt;_config.yml&lt;/code&gt; file of &lt;a href=&#34;https://github.com/ggplot2-exts/gallery&#34; target=&#34;_blank&#34;&gt;the repository&lt;/a&gt; with the meta data for your extension.&lt;/li&gt;
&lt;li&gt;Push your changes and create a pull request.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Acknowledgement&lt;/strong&gt;: Special thanks to &lt;a href=&#34;http://ryanhafen.com/&#34; target=&#34;_blank&#34;&gt;Dr. Ryan Hafen&lt;/a&gt; (&lt;a href=&#34;https://twitter.com/hafenstats&#34; target=&#34;_blank&#34;&gt;@hafenstats&lt;/a&gt;) for inspiring the design of this &lt;a href=&#34;http://www.ggplot2-exts.org/gallery/&#34; target=&#34;_blank&#34;&gt;Gallery page&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Webinar: Model-Based Machine Learning and Probabilistic Programming using RStan</title>
      <link>http://www.danielemaasit.com/post/2016/07/16/webinar-model-based-machine-learning-and-probabilistic-programming-using-rstan/</link>
      <pubDate>Sat, 16 Jul 2016 22:36:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/post/2016/07/16/webinar-model-based-machine-learning-and-probabilistic-programming-using-rstan/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://www.danielemaasit.com/img/mbml-webinar2.png&#34; alt=&#34;mbml-webinar2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I am glad to announce that I shall be presenting a live webinar with &lt;a href=&#34;https://www.dominodatalab.com/&#34; target=&#34;_blank&#34;&gt;Domino Data Labs&lt;/a&gt; on July 20, 2016 from 11:00 - 11:30 AM PST on &lt;a href=&#34;https://blog.dominodatalab.com/an-introduction-to-model-based-machine-learning/&#34; target=&#34;_blank&#34;&gt;Model-Based Machine Learning and Probabilistic Programming using RStan&lt;/a&gt;. If you are interested in adopting machine learning but are overwhelmed by the vast amount of learning algorithms, this webinar will show how to overcome that challenge. This &lt;a href=&#34;https://blog.dominodatalab.com/an-introduction-to-model-based-machine-learning/&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; describes most of the material we will cover in the webinar. Here is the &lt;a href=&#34;https://www.dominodatalab.com/resource/webinar/model-based-machine-learning&#34; target=&#34;_blank&#34;&gt;abstract&lt;/a&gt; for the webinar:&lt;/p&gt;

&lt;h2 id=&#34;synopsis&#34;&gt;Synopsis&lt;/h2&gt;

&lt;p&gt;In the last several decades, thousands of machine learning algorithms have been developed. Very often, the selection of an algorithm to solve a particular problem is driven more by the data scientist&amp;rsquo;s familiarity with a small subset of available algorithms, than optimizing for predictive power or operational constraints. This is unsurprising: Newcomers to machine learning and veteran data scientists alike, may be overwhelmed by the multitude of machine learning algorithms and where and how it is most appropriate to use them.&lt;/p&gt;

&lt;p&gt;In this webinar, Daniel Emaasit will introduce Model-Based Machine Learning (MBML), an approach to machine learning which addresses these challenges. Daniel will discuss the various uses of MBML, from tasks such as classification, to regression and clustering, and how it allows data scientists to address the uncretainty inherent to real-world machine learning applications. Daniel will demonstrate how to implement MBML in a probabilistic programming language called Stan, using the RStan package. At the end of webinar, attendees will have the knowledge to build their own custom probabilistic models, learning their parameters from data.&lt;/p&gt;

&lt;p&gt;Click this &lt;a href=&#34;https://www.dominodatalab.com/resource/webinar/model-based-machine-learning&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to register for the webinar. I look forward to seeing you there and answering your questions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Incase you missed it: My Talk at the United Nations Global Pulse Workshop</title>
      <link>http://www.danielemaasit.com/post/2016/07/07/incase-you-missed-it-my-talk-at-the-united-nations-global-pulse-worshop/</link>
      <pubDate>Thu, 07 Jul 2016 23:16:47 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/post/2016/07/07/incase-you-missed-it-my-talk-at-the-united-nations-global-pulse-worshop/</guid>
      <description>&lt;p&gt;In case you missed my talk at the &lt;a href=&#34;http://www.datascienceafrica.org/dsa2016/#workshop&#34; target=&#34;_blank&#34;&gt;2016 Data Science Africa Workshop&lt;/a&gt; organized by the &lt;a href=&#34;http://unglobalpulse.org/kampala&#34; target=&#34;_blank&#34;&gt;United Nations Global Pulse Lab&lt;/a&gt;, here is the recording. My talk was titled &amp;ldquo;&lt;em&gt;&lt;a href=&#34;https://danielemaasit.com/post/2016/06/28/a-preview-of-my-talk-for-the-data-science-africa-workshop-organized-by-the-united-nations/&#34; target=&#34;_blank&#34;&gt;Sustainable Urban Transport Planning using Big Data from Mobile Phones&lt;/a&gt;&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/47IjdD2yyGE&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;You can download slides for my talk from &lt;a href=&#34;https://www.dropbox.com/s/v53ymxth8x4hpe1/dsa_2016_presentation.pdf?dl=0&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/6RhjZngF83Q7r9&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; 

&lt;p&gt;There were also talks from my colleagues at &lt;a href=&#34;http://www.research.ibm.com/labs/africa/&#34; target=&#34;_blank&#34;&gt;IBM Research - Africa&lt;/a&gt; including Oliver Bent, Simone Fobi and Skyler Speakman who gave a general overview of the work we are doing in our lab in Nairobi, Kenya. Here is their recording:&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/gxFstDWf9VU&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;I would like to thank Dr. John Quinn and the entire team a the &lt;a href=&#34;http://unglobalpulse.org/kampala&#34; target=&#34;_blank&#34;&gt;UNGP - Kampala&lt;/a&gt; for the invitation to speak and for a well organized event.  You can read more about the workshop from this &lt;a href=&#34;http://unglobalpulse.org/news/data-science-in-africa-2016&#34; target=&#34;_blank&#34;&gt;blog post&lt;/a&gt; at the &lt;a href=&#34;http://unglobalpulse.org/blog&#34; target=&#34;_blank&#34;&gt;UNGP blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It was great meeting a lot of people from academia, industry, non-profits and government who are using Data Science to solve several challenges on the African continent, ranging from health care to agriculture and to sustainable cities. I look forward to attending more events in the future.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.danielemaasit.com/img/dsa-pic.jpg&#34; alt=&#34;dsa-pic&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Model-Based Machine Learning</title>
      <link>http://www.danielemaasit.com/talk/intro-to-mbml/</link>
      <pubDate>Wed, 06 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.danielemaasit.com/talk/intro-to-mbml/</guid>
      <description>

&lt;h3 id=&#34;pre-requisites&#34;&gt;Pre-requisites:&lt;/h3&gt;

&lt;p&gt;The following software tools are required to run the demo(s):&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;R + RStudio:- Follow this &lt;a href=&#34;https://www.r-project.org/&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to install R. Also install &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/preview/&#34; target=&#34;_blank&#34;&gt;RStudio&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;rstan:- Follow this &lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to install rstan.&lt;/li&gt;
&lt;li&gt;bayesplot:- Follow this &lt;a href=&#34;https://github.com/stan-dev/bayesplot&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to install bayesplot&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
