<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on </title>
    <link>/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Daniel Emaasit</copyright>
    <lastBuildDate>Mon, 14 Aug 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/machine-learning/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ICYMI: My talk on Introduction to Probabilistic Machine Learning</title>
      <link>/post/2017/08/14/introduction-to-probabilistic-machine-learning/</link>
      <pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/08/14/introduction-to-probabilistic-machine-learning/</guid>
      <description>&lt;p&gt;Incase you missed it, here is a recording of my talk on Introduction to Probabilisitic Machine Learning at the Las Vegas R &amp;amp; Data Science Meetup groups.&lt;/p&gt;
&lt;p&gt;I introduced probabilistic machine learning and probabilistic programming with Stan. I discussed the basics of machine learning from a probabilistic/Bayesian perspective and contrasted it with traditional/algorithmic machine learning. I also discussed how to build probabilistic models in computer code using a new exciting programming paradigm called Probabilistic Programming (PP). Particularly I used Stan (within R), a PP language, to build models ranging from simple generalized linear models to complex hierarchical models and nonparametric models for machine learning.&lt;/p&gt;
&lt;p&gt;Slides and code can be found on Github &lt;a href=&#34;https://bit.ly/intro-pml-lv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;part-i&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part I&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/ICFtztrK9a4&#34; frameborder=&#34;0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;div id=&#34;part-ii&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part II&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/YsGAce_3Ql4&#34; frameborder=&#34;0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Automating Hyperparameter tuning</title>
      <link>/project/auto-tdm/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/auto-tdm/</guid>
      <description>&lt;p&gt;Statistical estimation of integrated travel demand models (TDMs) is tedious and involves an iterative process which requires expert knowledge and time to perform several tasks including but not limited to: preprocessing data, selecting appropriate attributes, selecting appropriate models, finding model parameters, and critically analyzing results (Castiglione et al., 2015). Model estimation involves varying some free parameters, hyperparameters, to produce several models which are compared to select the most superior one. Considerable time and expertise are required to tune hyperparameters and consequently select appropriate models; which is prone to subjective error. The subjectivity of tuning hyperparameters for integrated TDMs makes published results difficult to reproduce, extend, and development is more an art than a science (Bergstra et al., 2011). There is a need for methods to minimize subjectivity in hyperparameter tuning and model selection.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
